{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRISM L1B & L2 Data Decomposed \n",
    "\n",
    "![PRISM logo](https://prism.jpl.nasa.gov/images/prism_banner2.png)\n",
    "\n",
    ">This notebook covers common raster data use patterns in the context of PRISM L1B and L2 imagery (with a twist to compensate for the grid rotation in the coordinate transforms).\n",
    ">\n",
    ">Its routines decompose the standard ENVI header file for a sample flight line near ([#](#)) on ([#](#)), then derives the metadata to store and fully document the corresponding image using the netCDF-4 format and community metadata standards (`CF-1.8, ACDD-1.3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#background\" data-toc-modified-id=\"background-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>background</a></span><ul class=\"toc-item\"><li><span><a href=\"#CF-grid-mappings\" data-toc-modified-id=\"CF-grid-mappings-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>CF grid mappings</a></span></li><li><span><a href=\"#PRISM-data-access\" data-toc-modified-id=\"PRISM-data-access-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>PRISM data access</a></span></li><li><span><a href=\"#requirements\" data-toc-modified-id=\"requirements-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>requirements</a></span></li><li><span><a href=\"#inputs\" data-toc-modified-id=\"inputs-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>inputs</a></span></li></ul></li><li><span><a href=\"#input-(ENVI-binary-image)\" data-toc-modified-id=\"input-(ENVI-binary-image)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>input (ENVI binary image)</a></span><ul class=\"toc-item\"><li><span><a href=\"#header\" data-toc-modified-id=\"header-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>header</a></span><ul class=\"toc-item\"><li><span><a href=\"#shape\" data-toc-modified-id=\"shape-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>shape</a></span></li><li><span><a href=\"#interleave\" data-toc-modified-id=\"interleave-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>interleave</a></span></li><li><span><a href=\"#data-type\" data-toc-modified-id=\"data-type-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>data type</a></span></li><li><span><a href=\"#byte-order\" data-toc-modified-id=\"byte-order-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>byte order</a></span></li><li><span><a href=\"#map-info\" data-toc-modified-id=\"map-info-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>map info</a></span></li><li><span><a href=\"#raster-transforms\" data-toc-modified-id=\"raster-transforms-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>raster transforms</a></span></li><li><span><a href=\"#longitudes,-latitudes\" data-toc-modified-id=\"longitudes,-latitudes-2.1.7\"><span class=\"toc-item-num\">2.1.7&nbsp;&nbsp;</span>longitudes, latitudes</a></span></li></ul></li><li><span><a href=\"#read-image\" data-toc-modified-id=\"read-image-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>read image</a></span></li></ul></li><li><span><a href=\"#output-(netCDF-4)\" data-toc-modified-id=\"output-(netCDF-4)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>output (netCDF-4)</a></span><ul class=\"toc-item\"><li><span><a href=\"#global-attributes\" data-toc-modified-id=\"global-attributes-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>global attributes</a></span></li><li><span><a href=\"#dimensions\" data-toc-modified-id=\"dimensions-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>dimensions</a></span></li><li><span><a href=\"#coordinate-variables\" data-toc-modified-id=\"coordinate-variables-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>coordinate variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#spatial\" data-toc-modified-id=\"spatial-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>spatial</a></span></li><li><span><a href=\"#bands\" data-toc-modified-id=\"bands-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>bands</a></span></li></ul></li><li><span><a href=\"#data\" data-toc-modified-id=\"data-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>data</a></span></li></ul></li><li><span><a href=\"#appendix\" data-toc-modified-id=\"appendix-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>appendix</a></span><ul class=\"toc-item\"><li><span><a href=\"#links\" data-toc-modified-id=\"links-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>links</a></span></li><li><span><a href=\"#tests\" data-toc-modified-id=\"tests-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>tests</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## background\n",
    "\n",
    "ENVI's binary raster format is convenient in some analysis contexts. But it's a chore to use in many others. So, PRISM L1B and L2 data should be made available using file formats and tools that allow it to be freely transformed/derived/etc to meet new research needs.\n",
    "\n",
    "We'll describe the spatial coverage for a PRISM dataset in binary image format using its header file `.hdr`. Then, we write the many-band image to a netCDF-4 with CF/ACDD metadata. \n",
    "\n",
    "### CF grid mappings\n",
    "\n",
    "A key compliance metric is a properly formed [*grid_mapping*](http://cfconventions.org/cf-conventions/cf-conventions.html#grid-mappings-and-projections) variable (introduced in CF-1.6). Standard grid mappings store CRS information as attributes of a data-less, dimension-less variable. The projection parameters depend on the `grid_mapping`. \n",
    "\n",
    "We claim success when we have PRISM L1B data from the ENVI file plotted as a georeferenced raster in Panoply, if it's currently capable. I think so. Here are a few worth trying:\n",
    "\n",
    "1. **transverse_mercator**, a CF standard grid mapping,\n",
    "\n",
    "```python\n",
    "char Transverse_mercator;\n",
    "  :grid_mapping_name = \"transverse_mercator\";\n",
    "  :longitude_of_central_meridian = -32.0; \n",
    "  :latitude_of_projection_origin = 40.0; \n",
    "  :scale_factor_at_central_meridian = 0.9330127018922193; \n",
    "  :false_easting = 0.0;\n",
    "  :false_northing = 0.0;\n",
    "  :semi_major_axis =  6378.137;\n",
    "  :semi_minor_axis =  6356.752;\n",
    "  :inverse_flattening =   298.257;\n",
    "  :_CoordinateTransformType = \"Projection\";\n",
    "  :_CoordinateAxisTypes = \"GeoX GeoY\";\n",
    "```\n",
    "\n",
    "2. **universal_transverse_mercator**, not a CF standard grid mapping but [recognized by netCDF-Java](https://www.unidata.ucar.edu/software/netcdf-java/v4.6/reference/StandardCoordinateTransforms.html), \n",
    "\n",
    "```python\n",
    "char UTM_Projection;\n",
    "  :grid_mapping_name = \"universal_transverse_mercator\";\n",
    "  :utm_zone_number = 22; \n",
    "  :semi_major_axis = 6378137;\n",
    "  :inverse_flattening = 298.257;\n",
    "  :_CoordinateTransformType = \"Projection\";\n",
    "  :_CoordinateAxisTypes = \"GeoX GeoY\";\n",
    "```\n",
    "\n",
    "3. **???**\n",
    "\n",
    "### PRISM data access \n",
    "\n",
    "Go see the [PRISM flight locator tool](https://prism.jpl.nasa.gov/dataportal/) if you haven't already. Or, run the cell below to render the web map here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe \n",
       "    src=\"https://nasa.maps.arcgis.com/apps/InteractiveFilter/index.html?appid=01bf106f07fc4bbab8464b2d04ad1e77\"\n",
       "    width=\"100%\"\n",
       "    height=\"650\"\n",
       "    frameborder=\"2\"\n",
       "    scrolling=\"no\"\n",
       "    marginheight=\"5\"\n",
       "    marginwidth=\"5\"\n",
       "    title=\"PRISM Flight Lines\">\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<iframe \n",
    "    src=\"https://nasa.maps.arcgis.com/apps/InteractiveFilter/index.html?appid=01bf106f07fc4bbab8464b2d04ad1e77\"\n",
    "    width=\"100%\"\n",
    "    height=\"650\"\n",
    "    frameborder=\"2\"\n",
    "    scrolling=\"no\"\n",
    "    marginheight=\"5\"\n",
    "    marginwidth=\"5\"\n",
    "    title=\"PRISM Flight Lines\">\n",
    "</iframe>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements\n",
    "\n",
    "Community software requirements include:\n",
    "\n",
    "* [numpy](https://numpy.org/doc/stable/index.html) -- `numpy` does most of everything except the coordinate transforms, even reads the binary file.\n",
    "* [netCDF4](https://unidata.github.io/netcdf4-python/netCDF4/index.html) -- for writing a beautiful netCDF-4 file\n",
    "* [pyproj](https://pyproj4.github.io/pyproj/stable/) -- can't calculate UTMs reliably with flight lines spread across all latitudes. PROJ, tho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "from pyproj.transformer import AreaOfInterest, Transformer\n",
    "from pyproj import crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining imports are from the Python 3 standard library:\n",
    "\n",
    "* **[tarfile](#)**: Provides a convenient context manager, methods for compressing/decompressing, methods to inspect tarball contents efficiently, and methods for read/write that allow selective access to tarball data content (`.tar.gz`).\n",
    "\n",
    "* **[io](#)**: Creates a memory buffer object to store the raw data we read from the files inside the tarball. The buffer object is effectively a *pseudo file*. I use it to access the raw data in memory without writing to disk first.\n",
    "\n",
    "* **[math](#)**: We use `pi` to convert degrees to radians (image rotation angle). We use `sin` and `cos` to convert the affine transform coefficients (that georeference the grid inside the native coordinate system) into a modified form that compensates for image rotation, which isn't compatible with the common linear transforms that GIS uses to place a north-up raster.\n",
    "\n",
    "* **[datetime](#)**: Provides reliable translation between POSIX formats for timestamp strings.\n",
    "\n",
    "* **[os.path](#)**: Provides information about the local filesystem. We use it to determine whether or not files/directories already exist (e.g. to avoid re-downloading PRISM granules that already exists locally).\n",
    "\n",
    "* **[urllib.request](#)**: My preferred method to access remote data that doesn't rely on non-standard Python packages.\n",
    "\n",
    "* **[shutil](#)**: Used once during an optional step that validates coefficients that describe the transform that renders the UTM coordinates `x,y` that correspond to the `i,j` indices for a target pixel.\n",
    "\n",
    "the forward transform from pixel coordinates `i,j` to UTM coordinates used in the native coordinate reference system.\n",
    "\n",
    "in an optional step to validate the transformation coefficients to warp  to determine if we have access to the `gdalinfo` command line utility. If so, it returns the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from io import TextIOWrapper\n",
    "from math import pi, sin, cos\n",
    "from datetime import datetime\n",
    "from os.path import basename, isfile\n",
    "from urllib.request import urlretrieve\n",
    "from shutil import which"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inputs\n",
    "\n",
    "**Use the web map to select your desired flight line.** Copy the url to the tarball for L2 reflectance into the cell below. Or just use the url from my example.\n",
    "\n",
    "(Slightly more complicated logic for L1B data is not set up yet.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prm20160125t195943_refl.tar.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__url__ = \"ftp://prmuser:tHQxi3a5@avng.jpl.nasa.gov/y16_data/prm20160125t195943_refl.tar.gz\"\n",
    "__tar__ = basename(__url__)\n",
    "__tar__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'prm20160125t195943_refl.tar.gz'. Skip download.\n"
     ]
    }
   ],
   "source": [
    "if isfile(__tar__):\n",
    "    print(f\"Found '{__tar__}'. Skip download.\")\n",
    "else:\n",
    "    print(f\"Downloading '{__url__}' ...\")\n",
    "    try:\n",
    "        urlretrieve(__url__, __tar__)\n",
    "    except Exception as e:\n",
    "        print(f\"Download fail!\")\n",
    "        raise e\n",
    "    else:\n",
    "        print(f\"Download success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tarfile's context manager to open the parent archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prm20160125t195943_corr_v1k',\n",
       " 'prm20160125t195943_corr_v1k/prm20160125t195943_corr_v1k_img',\n",
       " 'prm20160125t195943_corr_v1k/prm20160125t195943_corr_v1k_img.hdr',\n",
       " 'prm20160125t195943_corr_v1k/prm20160125t195943_README_v1k.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tarfile.open(__tar__, \"r\") as z:\n",
    "    tcontents = z.getnames()\n",
    "tcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll **call the path to the output netCDF `__out__`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prm20160125t195943_refl.nc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__out__ = __tar__.replace(\".tar.gz\", \".nc\")\n",
    "__out__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input (ENVI binary image)\n",
    "\n",
    "### header\n",
    "\n",
    "https://www.harrisgeospatial.com/docs/enviheaderfiles.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the header file from the `tar_contents` and read it as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _hpath = [t for t in tcontents if t.endswith(\"_img.hdr\")][0]\n",
    "except IndexError as e:\n",
    "    raise Exception(\"ERROR: No '.hdr' in source tarball. Exiting.\")\n",
    "else:\n",
    "    with tarfile.open(__tar__, \"r\") as z:\n",
    "        with z.extractfile(_hpath) as f:\n",
    "            hdr = TextIOWrapper(f, encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some nested loops to strip out all the white space and special characters, then reformat the header as a dictionary. Any arrays will become lists. These should work reliably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split the header into a list of lines. Drop the first one.\n",
    "hlines = hdr.split(\"\\n\")[1:]\n",
    "\n",
    "# Parsing header in a few nested loops. First, split keys from values.\n",
    "hpairs = [l.split(\" = \") for l in hlines if \" = \" in l]\n",
    "\n",
    "# Then, format the resulting strings as a dictionary.\n",
    "hdict = {l[0]: l[1].strip() for l in hpairs}\n",
    "\n",
    "# Parse the 'map info' into a labeled array.\n",
    "hdict['map info'] = [k.strip() for k in hdict['map info'][1:-1].split(\" , \")]\n",
    "\n",
    "# Iterate over a few special header elements and parse further.\n",
    "for k in ['wavelength', 'fwhm', 'correction factors', 'smoothing factors']:\n",
    "    hdict[k] = [float(v.strip()) for v in hdict[k][1:-1].split(\",\")]\n",
    "\n",
    "hdr = hdict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the keys of the header dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['description',\n",
       " 'samples',\n",
       " 'lines',\n",
       " 'bands',\n",
       " 'header offset',\n",
       " 'file type',\n",
       " 'data type',\n",
       " 'interleave',\n",
       " 'byte order',\n",
       " 'map info',\n",
       " 'wavelength units',\n",
       " 'smoothing factors',\n",
       " 'data ignore value',\n",
       " 'wavelength',\n",
       " 'fwhm',\n",
       " 'correction factors']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hdr.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shape\n",
    "\n",
    "Check the shape of the 3-dimensional gridded dataset, and convert the sizes of the dimensions to integers while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197, 246, 985)\n"
     ]
    }
   ],
   "source": [
    "samples = int(hdr['samples'])\n",
    "bands = int(hdr['bands'])\n",
    "lines = int(hdr['lines'])\n",
    "\n",
    "print((samples, bands, lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interleave\n",
    "\n",
    "Note the interleave types and the dimension orders. We need to reshape the giant 1-dimensional array that we read from the binary file in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197, 246, 985)\n"
     ]
    }
   ],
   "source": [
    "native_shape = {\n",
    "    'BSQ': (samples, lines, bands),  # Band Sequential\n",
    "    'BIP': (bands, samples, lines),  # Band Interleave by Pixel\n",
    "    'BIL': (samples, bands, lines),  # Band Interleave by Line\n",
    "}[hdr['interleave'].upper()]\n",
    "\n",
    "print(native_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BIL'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr['interleave'].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data type\n",
    "\n",
    "See the type map in the table on the ENVI header [documentation](https://www.harrisgeospatial.com/docs/enviheaderfiles.html). Also see the type codes given in the `numpy.dtypes` [documentation](https://numpy.org/doc/stable/reference/arrays.dtypes.html) (and [here](https://numpy.org/doc/stable/user/basics.types.html)).\n",
    "\n",
    "Determine the corresponding `numpy` data type of the binary array stored in the ENVI image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = {\n",
    "    '1': np.uint8,    # Byte: 8-bit unsigned integer\n",
    "    '2': np.int16,    # Integer: 16-bit signed integer\n",
    "    '3': np.int32,    # Long: 32-bit signed integer\n",
    "    '4': np.single,   # Floating-point: 32-bit single-precision\n",
    "    '5': np.double,   # Double-precision: 64-bit double-precision floating-point\n",
    "    '6': np.csingle,  # Complex: Real-imaginary pair of single-precision floating-point\n",
    "    '9': np.cdouble,  # Double-precision complex: Real-imaginary pair of double precision floating-point\n",
    "    '12': np.uint16,  # Unsigned integer: 16-bit\n",
    "    '13': np.uint32,  # Unsigned long integer: 32-bit\n",
    "    '14': np.int64,   # 64-bit long integer (signed)\n",
    "    '15': np.uint64,  # 64-bit unsigned long integer (unsigned)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up the data type of the example PRISM L2 binary image file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr['data type'] = data_types[hdr['data type']]\n",
    "hdr['data type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### byte order\n",
    "\n",
    "The `byte order` field conveys the order of the bytes in integer, long integer, 64-bit integer, unsigned 64-bit integer, floating point, double precision, and complex data types. Use one of the following:\n",
    "\n",
    "* **0**: little endian; (Host (Intel) in the Header Info dialog) is least significant byte first (LSF) data (DEC and MS-DOS systems).\n",
    "* **1**: big endian; (Network (IEEE) in the Header Info dialog) is most significant byte first (MSF) data (all other platforms).\n",
    "\n",
    "Map the byte order to the appropriate numpy encoding (prefixed to the data type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hbyteorder = {\n",
    "    '0': \"<\",  # little-endian\n",
    "    '1': \">\",  # big-endian\n",
    "}[hdr['byte order']]\n",
    "\n",
    "hbyteorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map info\n",
    "\n",
    "Finally, let's label the `map info` data in the header data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_info_labels = {\n",
    "    0:  (\"Projection name\", str),\n",
    "    1:  (\"Reference (tie point) pixel x location (in file coordinates)\", int),\n",
    "    2:  (\"Reference (tie point) pixel y location (in file coordinates)\", int),\n",
    "    3:  (\"Pixel easting\", float),\n",
    "    4:  (\"Pixel northing\", float),\n",
    "    5:  (\"x pixel size\", float),\n",
    "    6:  (\"y pixel size\", float),\n",
    "    7:  (\"Projection zone (UTM only)\", int),\n",
    "    8:  (\"North or South (UTM only)\", str),\n",
    "    9:  (\"Datum\", str),\n",
    "    10: (\"Units\", str),\n",
    "    11: (\"Rotation\", lambda x: float(x.split(\"=\")[1])),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the array of spatial characteristics and replace the `map info` key in the header data dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Projection name': 'UTM',\n",
       " 'Reference (tie point) pixel x location (in file coordinates)': 1,\n",
       " 'Reference (tie point) pixel y location (in file coordinates)': 1,\n",
       " 'Pixel easting': 478393.771278,\n",
       " 'Pixel northing': 2488191.16776,\n",
       " 'x pixel size': 10.3,\n",
       " 'y pixel size': 10.3,\n",
       " 'Projection zone (UTM only)': 19,\n",
       " 'North or South (UTM only)': 'South',\n",
       " 'Datum': 'WGS-84',\n",
       " 'Units': 'units=Meters',\n",
       " 'Rotation': -17.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr['map info'] = {v[0]: v[1](hdr['map info'][k]) for k, v in map_info_labels.items()}\n",
    "hdr['map info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is familiar info for raster data users. Add x, y, band shape to the dictionary above and that describes the grid completely for ENVI's purposes. \n",
    "\n",
    "Note that, this critical `Rotation` angle (degrees) is missing from the ENVI docs! wth! https://www.harrisgeospatial.com/docs/ENVIHeaderFiles.html (scroll to `map info`)\n",
    "\n",
    "netCDF + CF's grid mappings spec give recommendations for writing coordinate variables and other metadata georeference the data. We need to make *four* arrays of spatial coordinates to conform to grid mapping spec:\n",
    "\n",
    "* a 1d array of X coordinates in meters (UTM eastings),\n",
    "* a 1d array of Y coordinates in meters (UTM northings),\n",
    "* a 2d array of longitude coordinates in decimal degrees,\n",
    "* a 2d array of latitude coordinates in decimal degrees,\n",
    "\n",
    "Get the X,Y origin and resolution from the `map info` header field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478393.771278, 10.3, 2488191.16776, 10.3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the X and Y position of the raster origin in meters.\n",
    "xorigin = hdr['map info']['Pixel easting']\n",
    "yorigin = hdr['map info']['Pixel northing']\n",
    "\n",
    "# Get the X and Y dimensions of the pixels in meters.\n",
    "xresolution = hdr['map info']['x pixel size']\n",
    "yresolution = hdr['map info']['y pixel size']\n",
    "\n",
    "(xorigin, xresolution, yorigin, yresolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raster transforms\n",
    "\n",
    "*(These slides come from [an outstanding slide deck](https://cs184.eecs.berkeley.edu/uploads/lectures/04_transforms-1/04_transforms-1_slides.pdf) on basic transforms taught in a computer science course at UC Berkley.)*\n",
    "\n",
    "An affine transform describes the relationship between raster positions (sample, line) and georeferenced coordinates (x, y).\n",
    "\n",
    "<img src=\"docs/affine-transforms-ren-ng-ucberkley.png\" width=\"50%\" />\n",
    "\n",
    "Let's apply the We define the transformation using six coefficients. Because  modified coefficients for items 1, 2, 4, and 5:\n",
    "\n",
    "```python\n",
    "0. x origin      # (The origin refers to top-left corner of top-left pixel, in this case.)\n",
    "1. x resolution \n",
    "2. x rotation\n",
    "3. y origin\n",
    "4. y rotation\n",
    "5. y resolution\n",
    "```\n",
    "\n",
    "Get the six coefficients like GDAL does:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Header gives raster rotation in degrees, but we need it in radians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.29670597283903605"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation = (pi/180) * hdr['map info']['Rotation']\n",
    "rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRISM L1B and L2 datasets use a rotated grid - the lines/samples in an image do not face north, in most cases. \n",
    "\n",
    "ENVI's image processing tools and binary raster format are suited to analyzing imagery with a rotated grid. But GIS software like ESRI's ArcGIS kit do a poor job with rotated grids. GDAL's implementation of the affine transform obfuscates the rotation component because it rasters that don't face \"north-up\" are fairly uncommon.\n",
    "\n",
    "<img src=\"docs/rotation-matrix-ren-ng-ucberkley.png\" width=\"50%\" />\n",
    "\n",
    "Our approach modifies the common affine transform coefficients (specfically the ones for the x and y resolution and rotation) by appliying the rotation independently as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478393.771278,\n",
       " 9.849938986419266,\n",
       " 3.011428558644189,\n",
       " 2488191.16776,\n",
       " -3.011428558644189,\n",
       " 9.849938986419266)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = (xorigin, \n",
    "      cos(rotation)*xresolution,\n",
    "      -sin(rotation)*xresolution,\n",
    "      yorigin, \n",
    "      sin(rotation)*yresolution,\n",
    "      cos(rotation)*yresolution, )\n",
    "\n",
    "gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If GDAL is available, you can check the new coefficients by comparing them with the ones returned by the [gdalinfo](https://gdal.org/programs/gdalinfo.html#gdalinfo) command line utility.**\n",
    "\n",
    "The driver for ENVI read/write support does not support access using the VSI handlers. So, you can't validate without extracting the tarball. You can paste the following snippet into a new cell and run it to validate the modified transform against your local file.\n",
    "\n",
    "```shell\n",
    "gdalinfo $img | grep -A 2 GeoTransform\n",
    "```\n",
    "\n",
    "**Which yields identical transform coefficients:**\n",
    "\n",
    "```python\n",
    "GeoTransform =\n",
    "  478393.771278, 9.849938986419266, -3.011428558644189\n",
    "  2488191.16776, -3.011428558644189, -9.849938986419266\n",
    "```\n",
    "\n",
    "The new coefficients apply the correct transform, yielding native UTM coordinates  to compensate for our rotated grid.\n",
    "\n",
    "\n",
    "rotation  the  the  coordinates that transform the linear  normal linear transform that  the UTM X,Y coordinates, as illustrated in this nice fig from the ENVI docs (source: [https://www.harrisgeospatial.com/docs/OverviewMapInformationInENVI.html#Standard](https://www.harrisgeospatial.com/docs/OverviewMapInformationInENVI.html#Standard)):\n",
    "\n",
    "<img src=https://www.harrisgeospatial.com/docs/html/images/GeorectifyImagery/map_transformation_standard.gif />\n",
    "\n",
    "Print the first/last items in each of the coordinate arrays + their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (197):  478394 -  480324 \n",
      "Y (985): 2488191 - 2478499\n"
     ]
    }
   ],
   "source": [
    "x = np.array([gt[0]+i*gt[1] for i in range(0, samples)])\n",
    "y = np.array([gt[3]-i*gt[5] for i in range(0, lines)])\n",
    "\n",
    "print(f\"X ({len(x)}):  {round(x[0])} -  {round(x[-1])}\", \"\\n\"\n",
    "      f\"Y ({len(y)}): {round(y[0])} - {round(y[-1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y coordinates should not descend. We need to flip the array, and we may need to flip the entire dataset along the Y dimension, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2478498.8277973635, 2488191.16776)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.flip(y)\n",
    "\n",
    "y[0], y[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and last values in the new X and Y arrays are shown above in meters (UTM eastings and northings). But we actually need to shift them by one half pixel in both directions, like `XUTM+0.5*XRES`, `YUTM+0.5*YRES`. This will shift the Xs and Ys to the pixel centers. They were at the top left corners of the pixels before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478398.6962474932, 2478503.752766857)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [i+0.5*gt[1] for i in x]\n",
    "y = [j+0.5*gt[5] for j in y]\n",
    "\n",
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the next step, get two 2-dimensional arrays of X and Y coordinates by expanding column- and row-wise to reference every pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 197)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2d, y2d = np.meshgrid(x, y)\n",
    "\n",
    "x2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### longitudes, latitudes\n",
    "\n",
    "Now we need to get two 2-dimensional arrays of longitudes and latitudes that coincide with the permuted X,Y positions. Here's the proj4 string that represents the appropriate UTM zone and north/south hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTM 19 South:  '+proj=utm +zone=19 +south +datum=WGS84 +units=m +no_defs'\n"
     ]
    }
   ],
   "source": [
    "zone = hdr['map info']['Projection zone (UTM only)']\n",
    "hemi = hdr['map info']['North or South (UTM only)']\n",
    "\n",
    "# Format the proj4 string with the UTM zone and the ns hemisphere identifier.\n",
    "proj4 = f\"+proj=utm +zone={zone} +{hemi.lower()} +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "# Neatly print the UTM zone information and its proj4 string.\n",
    "print(f\"UTM {zone} {hemi}:  '{proj4}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also do it this way, by plugging the zone number into one of two EPSG codes based on the hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32719'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsg = f\"326{zone}\" if hemi is \"North\" else f\"327{zone}\"\n",
    "epsg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, apply transformation over the two new 2-dimensional X and Y arrays to render the corresponding longitude and latitude arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 197)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init the transform based on source and target projections.\n",
    "xform = Transformer.from_crs(f\"epsg:{epsg}\", \"epsg:4326\", always_xy=True)\n",
    "\n",
    "# Apply PROJ default transform utm>>geo to all X,Y coordinates.\n",
    "lon, lat = xform.transform(x2d, y2d)\n",
    "\n",
    "lon.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the longitudes and latitudes reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-69.512469462162 -67.80767543251973 -69.46494301605104 -67.72060562033546\n"
     ]
    }
   ],
   "source": [
    "print(lon.min(), lat.min(), lon.max(), lat.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOYAH! Now we can read the image data and write a netCDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note about converting UTM to LL without PROJ:**\n",
    "\n",
    "The logic to transform from UTM to geographic coordinates is quite dense (not as bad as the other way around though). This is the best resource for the maths that I've ever come across: https://earth-info.nga.mil/GandG/publications/tm8358.2/TM8358_2.pdf\n",
    "\n",
    "That deserves a bookmark, for sure. Those details aren't documented all together like that in many places.\n",
    "\n",
    "This is a snippet was translated into Python from some Java code published by IBM. I'll use it to eliminate the dependency on PROJ once I iron out the other more straightforward parts of the workflow.\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "def utmToLatLng(zone, easting, northing, northernHemisphere=True):\n",
    "    \"\"\"Transform a UTM coordinate to longitude, latitude in decimal degrees.\n",
    "    #\n",
    "    # This methodology is adapted from the java implementation documented here: \n",
    "    #  http://www.ibm.com/developerworks/java/library/j-coordconvert/index.html\n",
    "    #\n",
    "    # This was the first Python implementation that I found that doesn't depend\n",
    "    #  on external references to coordinate reference parameters, etc.\n",
    "    #\n",
    "    \"\"\"\n",
    "    \n",
    "    a = 6378137         # Earth's radius mapped to the WGS84 ellipsoid\n",
    "    e = 0.081819191     # Eccentricity\n",
    "    e1sq = 0.006739497  # \n",
    "    k0 = 0.9996         # Scale factor\n",
    "    \n",
    "    # Adjust the northing if the input zone is in the southern hemisphere.\n",
    "    if not northernHemisphere:\n",
    "        northing = 10000000 - northing\n",
    "    \n",
    "    # ...\n",
    "    arc = northing/k0\n",
    "    mu = arc/(a*(1 - math.pow(e, 2)/4.0 - 3*math.pow(e, 4)/64.0 - 5*math.pow(e, 6)/256.0))\n",
    "    \n",
    "    # ...\n",
    "    ei = (1 - math.pow((1 - e*e), (1/2.0))) / (1 + math.pow((1 - e * e), (1/2.0)))\n",
    "    \n",
    "    # ...\n",
    "    ca = 3*ei / 2 - 27*math.pow(ei, 3)/32.0\n",
    "    \n",
    "    # ...\n",
    "    cb = 21* math.pow(ei, 2)/16 - 55*math.pow(ei, 4)/32\n",
    "    cc = 151*math.pow(ei, 3)/96\n",
    "    cd = 1097*math.pow(ei, 4)/512\n",
    "    phi1 = mu + ca*math.sin(2*mu) + cb*math.sin(4*mu) + cc*math.sin(6*mu) + cd*math.sin(8*mu)\n",
    "    \n",
    "    # ...\n",
    "    n0 = a / math.pow((1 - math.pow((e*math.sin(phi1)), 2)), (1/2.0))\n",
    "\n",
    "    # ...\n",
    "    r0 = a * (1 - e*e)/math.pow((1 - math.pow((e*math.sin(phi1)), 2)), (3/2.0))\n",
    "    fact1 = n0*math.tan(phi1) / r0\n",
    "\n",
    "    # ...\n",
    "    _a1 = 500000 - easting\n",
    "    dd0 = _a1/(n0*k0)\n",
    "    fact2 = dd0*dd0/2\n",
    "\n",
    "    # ...\n",
    "    t0 = math.pow(math.tan(phi1), 2)\n",
    "    Q0 = e1sq*math.pow(math.cos(phi1), 2)\n",
    "    fact3 = (5 + 3*t0 + 10*Q0 - 4*Q0*Q0 - 9*e1sq)*math.pow(dd0, 4)/24\n",
    "\n",
    "    # ...\n",
    "    fact4 = (61 + 90*t0 + 298*Q0 + 45*t0*t0 - 252*e1sq - 3*Q0*Q0)*math.pow(dd0, 6)/720\n",
    "\n",
    "    # ...\n",
    "    lof1 = _a1/(n0*k0)\n",
    "    lof2 = (1 + 2*t0 + Q0)*math.pow(dd0, 3)/6.0\n",
    "    lof3 = (5 - 2*Q0 + 28*t0 - 3*math.pow(Q0, 2) + 8*e1sq + 24*math.pow(t0, 2))*math.pow(dd0, 5)/120\n",
    "    _a2 = (lof1 - lof2 + lof3) / math.cos(phi1)\n",
    "    _a3 = _a2*180 / math.pi\n",
    "\n",
    "    # Compute latitude coordinate. lat*-1 if southern hemisphere.\n",
    "    latitude = 180*(phi1 - fact1*(fact2 + fact3 + fact4))/math.pi\n",
    "    latitude = -latitude if not northernHemisphere else latitude\n",
    "    \n",
    "    # Compute longitude coordinate.\n",
    "    longitude = ((zone > 0) and (6*zone - 183.0) or 3.0) - _a3\n",
    "\n",
    "    return (latitude, longitude)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read image\n",
    "\n",
    "Read the binary array from the file suffixed with `_img` (from inside the tarball). Make sure to pass the numpy data type as a keyword argument to `np.frombuffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9999., -9999., -9999., ..., -9999., -9999., -9999.], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    _img = [t for t in tcontents if t.endswith(\"_img\")][0]\n",
    "except IndexError as e:\n",
    "    raise Exception(\"ERROR: No '.hdr' in source tarball. Exiting.\")\n",
    "else:\n",
    "    with tarfile.open(__tar__, \"r\") as z:\n",
    "         with z.extractfile(_img) as zimg:\n",
    "            arr = np.frombuffer(zimg.read(), dtype=hdr['data type'])\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the size of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47735070"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should equal the `samples * lines * bands` from the ENVI image header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples*bands*lines == arr.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the array to match dimensions ordered for BIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 985, 197)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = arr.reshape((246, 985, 197))\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr = arr.reshape(native_shape)\n",
    "#plt.imshow(__arr_[200,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output (netCDF-4)\n",
    "\n",
    "Now we're ready to write all of that information to a new netCDF file. Here's a direct link to the `Dataset` init options: https://unidata.github.io/netcdf4-python/netCDF4/index.html#netCDF4.Dataset.__init__.\n",
    "\n",
    "Open the new dataset for writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): \n",
       "    variables(dimensions): \n",
       "    groups: "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset(\n",
    "    __out__, \n",
    "    mode=\"w\",           # Open in write mode.\n",
    "    clobber=True,       # Overwrite the existing file, if necessary.\n",
    "    format=\"NETCDF4\",   # Write the output file in netCDF-4 format.\n",
    "    parallel=False,     # Enable parallel read/write. (Must be built with MPI support.)\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global attributes\n",
    "\n",
    "This attribute configuration is just a first draft. They mostly follow the example [L1B and L2 datasets published by OBDAAC for CORAL](https://oceancolor.gsfc.nasa.gov/projects/prism-coral/).\n",
    "\n",
    "Make a dictionary to store the global attribute information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts = dict(\n",
    "    id                       = \"10.5067/PRISM/#\",\n",
    "    naming_authority         = \"gov.nasa.jpl.prism\",\n",
    "    license                  = \"https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/\",\n",
    "    project                  = \"NASA PRISM\",\n",
    "    project_url              = \"https://prism.jpl.nasa.gov/\",\n",
    "    institution              = \"NASA Jet Propulsion Laboratory\",\n",
    "    instrument               = \"PRISM (Portable Remote Imaging SpectroMeter)\",\n",
    "    platform                 = \"G-IV (Gulfstream-IV)\",\n",
    "    Conventions              = \"CF-1.7\",\n",
    "    keywords_vocabulary      = \"GCMD Science Keywords\",\n",
    "    standard_name_vocabulary = \"CF Standard Names v72\",\n",
    "    processing_version       = \"V1.0\",\n",
    "    product_version          = \"v1w2\",\n",
    "    product_name             = basename(__out__),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add creator metadata recommended by CF and ACDD Conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['creator_name']         = \"PRISM Science Team\"\n",
    "atts['creator_role']         = \"group\"\n",
    "atts['creator_url']          = \"https://prism.jpl.nasa.gov\"\n",
    "atts['creator_email']        = \"sarah.r.lundeen@jpl.nasa.gov\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add publisher metadata recommended by CF and ACDD Conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['publisher_name']       = \"PRISM Science Team\"\n",
    "atts['publisher_role']       = \"group\"\n",
    "atts['publisher_url']        = \"https://prism.jpl.nasa.gov\"\n",
    "atts['publisher_email']      = \"sarah.r.lundeen@jpl.nasa.gov\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a reference to the file write/update time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['date_created']         = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "atts['date_updated']         = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add any remaining file-dependent attributes that are recommended by CF and ACDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['geospatial_lon_min']   = lon.min()\n",
    "atts['geospatial_lon_max']   = lon.max()\n",
    "atts['geospatial_lon_units'] = \"degrees_east\"\n",
    "atts['geospatial_lat_min']   = lat.min()\n",
    "atts['geospatial_lat_max']   = lat.max()\n",
    "atts['geospatial_lat_units'] = \"degrees_north\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Won't write any logic to determine a start/end range for now. Space/time search is pretty important for PRISM, as I'm sure any readers of this doc already know. So PODAAC would need to describe a representative observation period in those attributes to have complete metadata according to our requirements.\n",
    "\n",
    "Skip time attributes, then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atts['time_coverage_start'] = \"yyyy-mm-ddThh:mm:ssZ\"\n",
    "# atts['time_coverage_end']   = \"yyyy-mm-ddThh:mm:ssZ\"\n",
    "# atts['time_coverage_res']   = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimensions\n",
    "\n",
    "The `samples` and `lines` (both integers) are used to specify the size of the `x` and `y` dimensions in the output file, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): x(197), y(985), band(246)\n",
       "    variables(dimensions): \n",
       "    groups: "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdim = ds.createDimension('x', size=samples)\n",
    "ydim = ds.createDimension('y', size=lines)\n",
    "bdim = ds.createDimension('band', size=bands)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll add another dimension `nv` to accomodate a paired axis (for bounding the coordinates axes that label a non-standard dimension, if that makes sense...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nvdim = ds.createDimension('nv', size=2)\n",
    "#nvdim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coordinate variables\n",
    "\n",
    "I typically write the coordinate variables next, right after the dimensions are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spatial\n",
    "\n",
    "We'll store spatial coordinates at the top of the file, in the dataset root group. CF isn't clear about whether or not this is a requirement. So we play it safe.\n",
    "\n",
    "Add the `x` and `y` coordinates and attributes as new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): x(197), y(985), band(246)\n",
       "    variables(dimensions): float64 x(x), float64 y(y)\n",
       "    groups: "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_var = ds.createVariable('x', 'f8', ('x'), fill_value=None)\n",
    "x_var.units = \"m\"\n",
    "x_var.axis = \"X\"\n",
    "x_var.standard_name = \"projection_x_coordinate\"\n",
    "x_var.long_name = \"x coordinate of projection\"\n",
    "x_var[:] = x\n",
    "\n",
    "y_var = ds.createVariable('y', 'f8', ('y'), fill_value=None)\n",
    "y_var.units = \"m\"\n",
    "y_var.axis = \"Y\"\n",
    "y_var.standard_name = \"projection_y_coordinate\"\n",
    "y_var.long_name = \"y coordinate of projection\"\n",
    "y_var[:] = y\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longitude and latitude arrays are 2-dimensional. Make sure to order `y,x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): x(197), y(985), band(246)\n",
       "    variables(dimensions): float64 x(x), float64 y(y), float32 lat(y, x), float32 lon(y, x)\n",
       "    groups: "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_var = ds.createVariable('lat', 'f4', ('y', 'x'), fill_value=None)\n",
    "lat_var.units = \"degrees_north\"\n",
    "lat_var.standard_name = \"latitude\"\n",
    "lat_var.long_name = \"latitude coordinate\"\n",
    "lat_var[:,:] = lat\n",
    "\n",
    "lon_var = ds.createVariable('lon', 'f4', ('y', 'x'), fill_value=None)\n",
    "lon_var.units = \"degrees_east\"\n",
    "lon_var.standard_name = \"longitude\"\n",
    "lon_var.long_name = \"longitude coordinate\"\n",
    "lon_var[:,:] = lon\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now the sometimes-tricky CF grid mapping requirements...***\n",
    "\n",
    "Let's do the CF grid mapping variable now. It's be assigned attributes based on the spec for the `transverse_mercator` standard grid mapping (CF-1.6+). And [pyproj](#) is so legit for this. It can dump the CF standard attributes as a Python dictionary.\n",
    "\n",
    "You can find the grid mapping spec in clean format here: https://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/build/apf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crs_wkt': 'PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"World Geodetic System 1984\",ELLIPSOID[\"WGS 84\",6378137,298.257223563,LENGTHUNIT[\"metre\",1]],ID[\"EPSG\",6326]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8901]]],CONVERSION[\"UTM zone 19S\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-69,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",500000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",10000000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]],ID[\"EPSG\",17019]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]]]',\n",
       " 'semi_major_axis': 6378137.0,\n",
       " 'semi_minor_axis': 6356752.314245179,\n",
       " 'inverse_flattening': 298.257223563,\n",
       " 'reference_ellipsoid_name': 'WGS 84',\n",
       " 'longitude_of_prime_meridian': 0.0,\n",
       " 'prime_meridian_name': 'Greenwich',\n",
       " 'geographic_crs_name': 'unknown',\n",
       " 'horizontal_datum_name': 'World Geodetic System 1984',\n",
       " 'projected_crs_name': 'unknown',\n",
       " 'grid_mapping_name': 'transverse_mercator',\n",
       " 'latitude_of_projection_origin': 0.0,\n",
       " 'longitude_of_central_meridian': -69.0,\n",
       " 'false_easting': 500000.0,\n",
       " 'false_northing': 10000000.0,\n",
       " 'scale_factor_at_central_meridian': 0.9996}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_mapping_atts = crs.CRS(proj4).to_cf()\n",
    "grid_mapping_atts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CF-required grid_mapping variable and set its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "|S1 UTM_Projection()\n",
       "    crs_wkt: PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"World Geodetic System 1984\",ELLIPSOID[\"WGS 84\",6378137,298.257223563,LENGTHUNIT[\"metre\",1]],ID[\"EPSG\",6326]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8901]]],CONVERSION[\"UTM zone 19S\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-69,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",500000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",10000000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]],ID[\"EPSG\",17019]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]]]\n",
       "    semi_major_axis: 6378137.0\n",
       "    semi_minor_axis: 6356752.314245179\n",
       "    inverse_flattening: 298.257223563\n",
       "    reference_ellipsoid_name: WGS 84\n",
       "    longitude_of_prime_meridian: 0.0\n",
       "    prime_meridian_name: Greenwich\n",
       "    geographic_crs_name: unknown\n",
       "    horizontal_datum_name: World Geodetic System 1984\n",
       "    projected_crs_name: unknown\n",
       "    grid_mapping_name: transverse_mercator\n",
       "    latitude_of_projection_origin: 0.0\n",
       "    longitude_of_central_meridian: -69.0\n",
       "    false_easting: 500000.0\n",
       "    false_northing: 10000000.0\n",
       "    scale_factor_at_central_meridian: 0.9996\n",
       "    _CoordinateTransformType: Projection\n",
       "    _CoordinateAxisTypes: GeoY GeoX\n",
       "unlimited dimensions: \n",
       "current shape = ()\n",
       "filling on, default _FillValue of \u0000 used"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crs_var = ds.createVariable(\"UTM_Projection\", \"|S1\")\n",
    "crs_var.setncatts(grid_mapping_atts)\n",
    "crs_var._CoordinateTransformType = \"Projection\";\n",
    "crs_var._CoordinateAxisTypes = \"GeoY GeoX\";\n",
    "\n",
    "crs_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bands\n",
    "\n",
    "The band dimension `band` coordinates are coming from the ENVI header's `wavelength` field. There are a few other sensor-related variables, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "float32 wavelength(band)\n",
       "    long_name: wavelengths of band centers\n",
       "    units: nm\n",
       "    valid_min: 350.0\n",
       "    valid_max: 1050.0\n",
       "unlimited dimensions: \n",
       "current shape = (246,)\n",
       "filling on, default _FillValue of 9.969209968386869e+36 used"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the array of wavelengths taken from the header.\n",
    "wave = np.array(hdr['wavelength'])\n",
    "\n",
    "# Create a wavelength variable; add the attributes and data.\n",
    "wave_var = ds.createVariable('wavelength', 'f4', ('band'), fill_value=None)\n",
    "wave_var.long_name = \"wavelengths of band centers\"\n",
    "wave_var.units = \"nm\"\n",
    "wave_var.valid_min = 350.0\n",
    "wave_var.valid_max = 1050.0\n",
    "wave_var[:] = wave\n",
    "\n",
    "wave_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good habit to bound the coordinates that describe dimensions without standard encodings. I don't know how to derive the precise band coverages, so here I'll just do something super dumb and +- 1 from each band center to create a placeholder 2d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a 2d array of wavelength_bnds coordinates.\n",
    "# wavebnds = np.array([[w-1., w+1.] for w in wave])\n",
    "\n",
    "# # Create a 'wavelength_bnds' variable to bound the sensor bands.\n",
    "# wavebnds_var = ds.createVariable(\"wavelength_bnds\", \"f4\", (\"band\", \"nv\"), fill_value=None)\n",
    "# wavebnds_var.units = \"nm\"\n",
    "# wavebnds_var[:,:] = wavebnds\n",
    "\n",
    "# wavebnds_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Re-visit the `fwhm` data (band full-width half-maximums). Array size doesn't match the other two (*wavelength*, *correction factors*). Ask M. Gierach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fwhm_var = ds.createVariable('fwhm', 'float32', ('band'))\n",
    "# fwhm_var.long_name = \"band full-width half-maximums\"\n",
    "# fwhm_var.units = \"nm\"\n",
    "# fwhm_var.valid_min = 349.9\n",
    "# fwhm_var.valid_max = 1053.5\n",
    "# fwhm_var[:] = np.array(hdr['fwhm'])\n",
    "\n",
    "# fwhm_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know what these *correction factors* describe. But they fit the band dimension *band*, so we add them as a sensor variable as assume they're unitless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the array so we can get real min, max to bound this variable.\n",
    "corr = np.array(hdr['correction factors'])\n",
    "\n",
    "# Make the 'correction factors' variable.\n",
    "corr_var = ds.createVariable('correction_factors', 'float32', ('band'))\n",
    "corr_var.long_name = \"correction factors\"\n",
    "corr_var.units = \"unitless\"\n",
    "corr_var.valid_min = corr.min()\n",
    "corr_var.valid_max = corr.max()\n",
    "corr_var[:] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data\n",
    "\n",
    "This next cell simply selects the attributes that are dependent on processing levels and updates the global attributes as needed. Hang on to the data variable attributes `data_atts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today we're translating 'reflectance' data from ENVI image format to netCDF-4.\n"
     ]
    }
   ],
   "source": [
    "if \"_refl\" in __tar__:\n",
    "    atts.update({'title': \"PRISM Level-2 Reflectance\", 'processing_level': \"L2\"})\n",
    "    data_name = \"reflectance\"\n",
    "    data_atts = {\n",
    "        'long_name': \"reflectance\",\n",
    "        'units': \"unitless\",\n",
    "        'valid_min': 0.0,\n",
    "        'valid_max': 1000.0,\n",
    "        'grid_mapping': 'UTM_Projection',\n",
    "        'coordinates': \"lat lon\",\n",
    "    }\n",
    "else:\n",
    "    atts.update({'title': \"PRISM Level-1B Orthocorrected Radiance\", 'processing_level': \"L1B\"})\n",
    "    data_name = \"radiance\"\n",
    "    data_atts = {\n",
    "        'long_name': \"at-sensor radiance\",\n",
    "        'units': \"uW cm^-2 nm^-1 sr^-1\",\n",
    "        'valid_min': 0.0,\n",
    "        'valid_max': 800.0,\n",
    "        'grid_mapping': 'UTM_Projection',\n",
    "        'coordinates': \"lat lon\",\n",
    "    }\n",
    "\n",
    "print(f\"Today we're translating '{data_name}' data from ENVI image format to netCDF-4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data variable can often be compressed for some space savings, so the setup is a bit different this time. After the variable name, set\n",
    "\n",
    "1. Data Type will be `f4` for most data variables in the PRISM outputs, I think.\n",
    "2. Dimensions should be `y`, `x`, `band`, in that order.\n",
    "3. ZLIB will attempt to apply compression.\n",
    "4. The default compression level `complevel` is four, on a scale from 0-9.\n",
    "5. The `_FillValue` attribute MUST be set when you make the variable. It cannot be set like other attributes. I can explain the details for any interested readers outside of this notebook.\n",
    "\n",
    "Make the new variable for the observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "float32 reflectance(band, y, x)\n",
       "    _FillValue: -9999.0\n",
       "    long_name: reflectance\n",
       "    units: unitless\n",
       "    valid_min: 0.0\n",
       "    valid_max: 1000.0\n",
       "    grid_mapping: UTM_Projection\n",
       "    coordinates: lat lon\n",
       "unlimited dimensions: \n",
       "current shape = (246, 985, 197)\n",
       "filling on"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure fill value parsed from ENVI header is same dtype as the data array.\n",
    "data_type = hdr['data type']\n",
    "fill_value = data_type( hdr['data ignore value'] )\n",
    "\n",
    "# Create the new variable in the 'observation_data' group.\n",
    "data = ds.createVariable(\n",
    "    data_name,\n",
    "    data_type,\n",
    "    dimensions=('band', 'y', 'x'),\n",
    "    zlib=True,\n",
    "    complevel=4,\n",
    "    fill_value=fill_value,\n",
    ")\n",
    "\n",
    "# Set the attributes for the variable.\n",
    "data.setncatts(data_atts)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now insert the array into the variable, expanding to all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "float32 reflectance(band, y, x)\n",
       "    _FillValue: -9999.0\n",
       "    long_name: reflectance\n",
       "    units: unitless\n",
       "    valid_min: 0.0\n",
       "    valid_max: 1000.0\n",
       "    grid_mapping: UTM_Projection\n",
       "    coordinates: lat lon\n",
       "unlimited dimensions: \n",
       "current shape = (246, 985, 197)\n",
       "filling on"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,:,:] = arr  #.reshape((bands, lines, samples))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, set global attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.setncatts(atts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the dataset and you're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the netCDF-4 C library utilities installed, you can call ncdump to eyeball the formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netcdf prm20160125t195943_refl {\r\n",
      "dimensions:\r\n",
      "\tx = 197 ;\r\n",
      "\ty = 985 ;\r\n",
      "\tband = 246 ;\r\n",
      "variables:\r\n",
      "\tdouble x(x) ;\r\n",
      "\t\tx:units = \"m\" ;\r\n",
      "\t\tx:axis = \"X\" ;\r\n",
      "\t\tx:standard_name = \"projection_x_coordinate\" ;\r\n",
      "\t\tx:long_name = \"x coordinate of projection\" ;\r\n",
      "\tdouble y(y) ;\r\n",
      "\t\ty:units = \"m\" ;\r\n",
      "\t\ty:axis = \"Y\" ;\r\n",
      "\t\ty:standard_name = \"projection_y_coordinate\" ;\r\n",
      "\t\ty:long_name = \"y coordinate of projection\" ;\r\n",
      "\tfloat lat(y, x) ;\r\n",
      "\t\tlat:units = \"degrees_north\" ;\r\n",
      "\t\tlat:standard_name = \"latitude\" ;\r\n",
      "\t\tlat:long_name = \"latitude coordinate\" ;\r\n",
      "\tfloat lon(y, x) ;\r\n",
      "\t\tlon:units = \"degrees_east\" ;\r\n",
      "\t\tlon:standard_name = \"longitude\" ;\r\n",
      "\t\tlon:long_name = \"longitude coordinate\" ;\r\n",
      "\tchar UTM_Projection ;\r\n",
      "\t\tUTM_Projection:crs_wkt = \"PROJCRS[\\\"unknown\\\",BASEGEOGCRS[\\\"unknown\\\",DATUM[\\\"World Geodetic System 1984\\\",ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,LENGTHUNIT[\\\"metre\\\",1]],ID[\\\"EPSG\\\",6326]],PRIMEM[\\\"Greenwich\\\",0,ANGLEUNIT[\\\"degree\\\",0.0174532925199433],ID[\\\"EPSG\\\",8901]]],CONVERSION[\\\"UTM zone 19S\\\",METHOD[\\\"Transverse Mercator\\\",ID[\\\"EPSG\\\",9807]],PARAMETER[\\\"Latitude of natural origin\\\",0,ANGLEUNIT[\\\"degree\\\",0.0174532925199433],ID[\\\"EPSG\\\",8801]],PARAMETER[\\\"Longitude of natural origin\\\",-69,ANGLEUNIT[\\\"degree\\\",0.0174532925199433],ID[\\\"EPSG\\\",8802]],PARAMETER[\\\"Scale factor at natural origin\\\",0.9996,SCALEUNIT[\\\"unity\\\",1],ID[\\\"EPSG\\\",8805]],PARAMETER[\\\"False easting\\\",500000,LENGTHUNIT[\\\"metre\\\",1],ID[\\\"EPSG\\\",8806]],PARAMETER[\\\"False northing\\\",10000000,LENGTHUNIT[\\\"metre\\\",1],ID[\\\"EPSG\\\",8807]],ID[\\\"EPSG\\\",17019]],CS[Cartesian,2],AXIS[\\\"(E)\\\",east,ORDER[1],LENGTHUNIT[\\\"metre\\\",1,ID[\\\"EPSG\\\",9001]]],AXIS[\\\"(N)\\\",north,ORDER[2],LENGTHUNIT[\\\"metre\\\",1,ID[\\\"EPSG\\\",9001]]]]\" ;\r\n",
      "\t\tUTM_Projection:semi_major_axis = 6378137. ;\r\n",
      "\t\tUTM_Projection:semi_minor_axis = 6356752.31424518 ;\r\n",
      "\t\tUTM_Projection:inverse_flattening = 298.257223563 ;\r\n",
      "\t\tUTM_Projection:reference_ellipsoid_name = \"WGS 84\" ;\r\n",
      "\t\tUTM_Projection:longitude_of_prime_meridian = 0. ;\r\n",
      "\t\tUTM_Projection:prime_meridian_name = \"Greenwich\" ;\r\n",
      "\t\tUTM_Projection:geographic_crs_name = \"unknown\" ;\r\n",
      "\t\tUTM_Projection:horizontal_datum_name = \"World Geodetic System 1984\" ;\r\n",
      "\t\tUTM_Projection:projected_crs_name = \"unknown\" ;\r\n",
      "\t\tUTM_Projection:grid_mapping_name = \"transverse_mercator\" ;\r\n",
      "\t\tUTM_Projection:latitude_of_projection_origin = 0. ;\r\n",
      "\t\tUTM_Projection:longitude_of_central_meridian = -69. ;\r\n",
      "\t\tUTM_Projection:false_easting = 500000. ;\r\n",
      "\t\tUTM_Projection:false_northing = 10000000. ;\r\n",
      "\t\tUTM_Projection:scale_factor_at_central_meridian = 0.9996 ;\r\n",
      "\t\tUTM_Projection:_CoordinateTransformType = \"Projection\" ;\r\n",
      "\t\tUTM_Projection:_CoordinateAxisTypes = \"GeoY GeoX\" ;\r\n",
      "\tfloat wavelength(band) ;\r\n",
      "\t\twavelength:long_name = \"wavelengths of band centers\" ;\r\n",
      "\t\twavelength:units = \"nm\" ;\r\n",
      "\t\twavelength:valid_min = 350.f ;\r\n",
      "\t\twavelength:valid_max = 1050.f ;\r\n",
      "\tfloat correction_factors(band) ;\r\n",
      "\t\tcorrection_factors:long_name = \"correction factors\" ;\r\n",
      "\t\tcorrection_factors:units = \"unitless\" ;\r\n",
      "\t\tcorrection_factors:valid_min = 1.f ;\r\n",
      "\t\tcorrection_factors:valid_max = 1.f ;\r\n",
      "\tfloat reflectance(band, y, x) ;\r\n",
      "\t\treflectance:_FillValue = -9999.f ;\r\n",
      "\t\treflectance:long_name = \"reflectance\" ;\r\n",
      "\t\treflectance:units = \"unitless\" ;\r\n",
      "\t\treflectance:valid_min = 0. ;\r\n",
      "\t\treflectance:valid_max = 1000. ;\r\n",
      "\t\treflectance:grid_mapping = \"UTM_Projection\" ;\r\n",
      "\t\treflectance:coordinates = \"lat lon\" ;\r\n",
      "\r\n",
      "// global attributes:\r\n",
      "\t\t:id = \"10.5067/PRISM/#\" ;\r\n",
      "\t\t:naming_authority = \"gov.nasa.jpl.prism\" ;\r\n",
      "\t\t:license = \"https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/\" ;\r\n",
      "\t\t:project = \"NASA PRISM\" ;\r\n",
      "\t\t:project_url = \"https://prism.jpl.nasa.gov/\" ;\r\n",
      "\t\t:institution = \"NASA Jet Propulsion Laboratory\" ;\r\n",
      "\t\t:instrument = \"PRISM (Portable Remote Imaging SpectroMeter)\" ;\r\n",
      "\t\t:platform = \"G-IV (Gulfstream-IV)\" ;\r\n",
      "\t\t:Conventions = \"CF-1.7\" ;\r\n",
      "\t\t:keywords_vocabulary = \"GCMD Science Keywords\" ;\r\n",
      "\t\t:standard_name_vocabulary = \"CF Standard Names v72\" ;\r\n",
      "\t\t:processing_version = \"V1.0\" ;\r\n",
      "\t\t:product_version = \"v1w2\" ;\r\n",
      "\t\t:product_name = \"prm20160125t195943_refl.nc\" ;\r\n",
      "\t\t:creator_name = \"PRISM Science Team\" ;\r\n",
      "\t\t:creator_role = \"group\" ;\r\n",
      "\t\t:creator_url = \"https://prism.jpl.nasa.gov\" ;\r\n",
      "\t\t:creator_email = \"sarah.r.lundeen@jpl.nasa.gov\" ;\r\n",
      "\t\t:publisher_name = \"PRISM Science Team\" ;\r\n",
      "\t\t:publisher_role = \"group\" ;\r\n",
      "\t\t:publisher_url = \"https://prism.jpl.nasa.gov\" ;\r\n",
      "\t\t:publisher_email = \"sarah.r.lundeen@jpl.nasa.gov\" ;\r\n",
      "\t\t:date_created = \"2020-08-23T20:00:22Z\" ;\r\n",
      "\t\t:date_updated = \"2020-08-23T20:00:22Z\" ;\r\n",
      "\t\t:geospatial_lon_min = -69.512469462162 ;\r\n",
      "\t\t:geospatial_lon_max = -69.464943016051 ;\r\n",
      "\t\t:geospatial_lon_units = \"degrees_east\" ;\r\n",
      "\t\t:geospatial_lat_min = -67.8076754325197 ;\r\n",
      "\t\t:geospatial_lat_max = -67.7206056203355 ;\r\n",
      "\t\t:geospatial_lat_units = \"degrees_north\" ;\r\n",
      "\t\t:title = \"PRISM Level-2 Reflectance\" ;\r\n",
      "\t\t:processing_level = \"L2\" ;\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!ncdump -h $__out__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## appendix\n",
    "\n",
    "### links\n",
    "\n",
    "When there's time, I'll add citations to the workflow based on the following links:\n",
    "\n",
    "1. [Climate and Forecast Conventions version 1.x](#)\n",
    "2. [Attribute Conventions for Data Discovery version 1.x](#)\n",
    "3. [NetCDF Attribute Convention for Dataset Discovery](https://www.unidata.ucar.edu/software/netcdf-java/v4.6/metadata/DataDiscoveryAttConvention.html)\n",
    "4. [standard_name_vocabulary](https://www.unidata.ucar.edu/software/netcdf-java/v4.6/metadata/DataDiscoveryAttConvention.html#standard_name_vocabulary_Attribute)\n",
    "5. https://earth-info.nga.mil/GandG/publications/tm8358.2/TM8358_2.pdf\n",
    "6. https://www.sciencedirect.com/science/article/pii/0098300480900151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests\n",
    "\n",
    "**Tests aren't implemented yet.** In fact, this workflow was implemented from memory without any end-to-end testing, so there are almost certainly bugs. Please let me validate before using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 192204\r\n",
      "drwxr-x--- 2 jack jack      4096 May 30  2017 prm20160125t195943_corr_v1k\r\n",
      "drwxr-xr-x 4 jack jack      4096 Aug 20 23:11 ..\r\n",
      "drwxr-xr-x 2 jack jack      4096 Aug 20 23:12 .ipynb_checkpoints\r\n",
      "-rw-r--r-- 1 jack jack 104050844 Aug 21 05:03 prm20160125t195943_refl.tar.gz\r\n",
      "-rw-r--r-- 1 jack jack      1820 Aug 21 05:24 .gitignore\r\n",
      "-rw-r--r-- 1 jack jack       599 Aug 21 12:35 README.md\r\n",
      "drwxr-xr-x 8 jack jack      4096 Aug 21 13:36 .git\r\n",
      "-rw-r--r-- 1 jack jack    393353 Aug 22 05:19 rotation-matrix.png\r\n",
      "drwxr-xr-x 2 jack jack      4096 Aug 22 06:00 docs\r\n",
      "drwxr-xr-x 6 jack jack      4096 Aug 22 06:40 .dev\r\n",
      "-rw-r--r-- 1 jack jack     81404 Aug 23 15:58 PRISM-Data-Decomposed.ipynb\r\n",
      "drwxr-xr-x 7 jack jack      4096 Aug 23 15:58 .\r\n",
      "-rw-r--r-- 1 jack jack  92243984 Aug 23 16:00 prm20160125t195943_refl.nc\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /home/jack/prm20160125t195943_refl.nc\n",
    "!cp prm20160125t195943_refl.nc ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(lon.min(), lat.min(), lon.max(), lat.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xarray as xr\n",
    "#ds1 = xr.open_dataset(\"docs/prm20160125t195943_corr_v1k_img.nc\")\n",
    "#ds2 = xr.open_dataset(\"prm20160125t195943_refl.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.Band200.plot(figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.lat.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2.lon.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.x.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2.x.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.y.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2.y.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.lat.data[:5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2.lat.data[:5, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.lon.data[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2.lon.data[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2['reflectance'].isel(band=200).plot(figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.close(), ds2.close()\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.plot(x2d[0,:])\n",
    "#plt.plot(x2d[1,:])\n",
    "#plt.plot(x2d[2,:])\n",
    "#plt.plot(y2d[:,0])\n",
    "#plt.plot(y2d[:,1])\n",
    "#plt.plot(y2d[:,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.48px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
