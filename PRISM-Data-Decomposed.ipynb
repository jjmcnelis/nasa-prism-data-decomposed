{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRISM L1B & L2 Data Decomposed \n",
    "\n",
    "![PRISM logo](https://prism.jpl.nasa.gov/images/prism_banner2.png)\n",
    "\n",
    ">\n",
    ">This notebook covers common raster data use patterns in the context of PRISM L1B and L2 imagery (with a twist to compensate for the grid rotation in the coordinate transforms).\n",
    ">\n",
    ">Its routines decompose the standard ENVI header file for a sample flight line near ([#](#)) on ([#](#)), then derives the metadata to store and fully document the corresponding image using the netCDF-4 format and community metadata standards (`CF-1.8, ACDD-1.3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#background\" data-toc-modified-id=\"background-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>background</a></span><ul class=\"toc-item\"><li><span><a href=\"#CF-grid-mappings\" data-toc-modified-id=\"CF-grid-mappings-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>CF grid mappings</a></span></li><li><span><a href=\"#PRISM-data-access\" data-toc-modified-id=\"PRISM-data-access-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>PRISM data access</a></span></li><li><span><a href=\"#requirements\" data-toc-modified-id=\"requirements-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>requirements</a></span></li><li><span><a href=\"#inputs\" data-toc-modified-id=\"inputs-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>inputs</a></span></li></ul></li><li><span><a href=\"#input-(ENVI-binary-image)\" data-toc-modified-id=\"input-(ENVI-binary-image)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>input (ENVI binary image)</a></span><ul class=\"toc-item\"><li><span><a href=\"#header\" data-toc-modified-id=\"header-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>header</a></span><ul class=\"toc-item\"><li><span><a href=\"#shape\" data-toc-modified-id=\"shape-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>shape</a></span></li><li><span><a href=\"#interleave\" data-toc-modified-id=\"interleave-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>interleave</a></span></li><li><span><a href=\"#data-type\" data-toc-modified-id=\"data-type-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>data type</a></span></li><li><span><a href=\"#byte-order\" data-toc-modified-id=\"byte-order-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>byte order</a></span></li><li><span><a href=\"#map-info\" data-toc-modified-id=\"map-info-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>map info</a></span></li><li><span><a href=\"#longitudes,-latitudes\" data-toc-modified-id=\"longitudes,-latitudes-2.1.6\"><span class=\"toc-item-num\">2.1.6&nbsp;&nbsp;</span>longitudes, latitudes</a></span></li></ul></li><li><span><a href=\"#read-image\" data-toc-modified-id=\"read-image-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>read image</a></span></li></ul></li><li><span><a href=\"#output-(netCDF-4)\" data-toc-modified-id=\"output-(netCDF-4)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>output (netCDF-4)</a></span><ul class=\"toc-item\"><li><span><a href=\"#global-attributes\" data-toc-modified-id=\"global-attributes-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>global attributes</a></span></li><li><span><a href=\"#dimensions\" data-toc-modified-id=\"dimensions-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>dimensions</a></span></li><li><span><a href=\"#coordinate-variables\" data-toc-modified-id=\"coordinate-variables-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>coordinate variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#spatial\" data-toc-modified-id=\"spatial-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>spatial</a></span></li><li><span><a href=\"#bands\" data-toc-modified-id=\"bands-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>bands</a></span></li></ul></li><li><span><a href=\"#data\" data-toc-modified-id=\"data-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>data</a></span></li></ul></li><li><span><a href=\"#appendix\" data-toc-modified-id=\"appendix-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>appendix</a></span><ul class=\"toc-item\"><li><span><a href=\"#links\" data-toc-modified-id=\"links-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>links</a></span></li><li><span><a href=\"#tests\" data-toc-modified-id=\"tests-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>tests</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## background\n",
    "\n",
    "ENVI's binary raster format is convenient in some analysis contexts. But it's a chore to use in many others. So, PRISM L1B and L2 data should be made available using file formats and tools that allow it to be freely transformed/derived/etc to meet new research needs.\n",
    "\n",
    "We'll describe the spatial coverage for a PRISM dataset in binary image format using its header file `.hdr`. Then, we write the many-band image to a netCDF-4 with CF/ACDD metadata. \n",
    "\n",
    "### CF grid mappings\n",
    "\n",
    "A key compliance metric is a properly formed [*grid_mapping*](http://cfconventions.org/cf-conventions/cf-conventions.html#grid-mappings-and-projections) variable (introduced in CF-1.6). Standard grid mappings store CRS information as attributes of a data-less, dimension-less variable. The projection parameters depend on the `grid_mapping`. \n",
    "\n",
    "We claim success when we have PRISM L1B data from the ENVI file plotted as a georeferenced raster in Panoply, if it's currently capable. I think so. Here are a few worth trying:\n",
    "\n",
    "1. **transverse_mercator**, a CF standard grid mapping,\n",
    "\n",
    "```python\n",
    "char Transverse_mercator;\n",
    "  :grid_mapping_name = \"transverse_mercator\";\n",
    "  :longitude_of_central_meridian = -32.0; \n",
    "  :latitude_of_projection_origin = 40.0; \n",
    "  :scale_factor_at_central_meridian = 0.9330127018922193; \n",
    "  :false_easting = 0.0;\n",
    "  :false_northing = 0.0;\n",
    "  :semi_major_axis =  6378.137;\n",
    "  :semi_minor_axis =  6356.752;\n",
    "  :inverse_flattening =   298.257;\n",
    "  :_CoordinateTransformType = \"Projection\";\n",
    "  :_CoordinateAxisTypes = \"GeoX GeoY\";\n",
    "```\n",
    "\n",
    "2. **universal_transverse_mercator**, not a CF standard grid mapping but [recognized by netCDF-Java](https://www.unidata.ucar.edu/software/netcdf-java/v4.6/reference/StandardCoordinateTransforms.html), \n",
    "\n",
    "```python\n",
    "char UTM_Projection;\n",
    "  :grid_mapping_name = \"universal_transverse_mercator\";\n",
    "  :utm_zone_number = 22; \n",
    "  :semi_major_axis = 6378137;\n",
    "  :inverse_flattening = 298.257;\n",
    "  :_CoordinateTransformType = \"Projection\";\n",
    "  :_CoordinateAxisTypes = \"GeoX GeoY\";\n",
    "```\n",
    "\n",
    "3. **???**\n",
    "\n",
    "### PRISM data access \n",
    "\n",
    "Go see the [PRISM flight locator tool](https://prism.jpl.nasa.gov/dataportal/) if you haven't already. Or, run the cell below to render the web map here in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe \n",
       "    src=\"https://nasa.maps.arcgis.com/apps/InteractiveFilter/index.html?appid=01bf106f07fc4bbab8464b2d04ad1e77\"\n",
       "    width=\"100%\"\n",
       "    height=\"650\"\n",
       "    frameborder=\"2\"\n",
       "    scrolling=\"no\"\n",
       "    marginheight=\"5\"\n",
       "    marginwidth=\"5\"\n",
       "    title=\"PRISM Flight Lines\">\n",
       "</iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "<iframe \n",
    "    src=\"https://nasa.maps.arcgis.com/apps/InteractiveFilter/index.html?appid=01bf106f07fc4bbab8464b2d04ad1e77\"\n",
    "    width=\"100%\"\n",
    "    height=\"650\"\n",
    "    frameborder=\"2\"\n",
    "    scrolling=\"no\"\n",
    "    marginheight=\"5\"\n",
    "    marginwidth=\"5\"\n",
    "    title=\"PRISM Flight Lines\">\n",
    "</iframe>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirements\n",
    "\n",
    "Community software requirements include:\n",
    "\n",
    "* [numpy](https://numpy.org/doc/stable/index.html) -- `numpy` does most of everything except the coordinate transforms, even reads the binary file.\n",
    "* [pyproj](https://pyproj4.github.io/pyproj/stable/) -- can't calculate UTMs reliably with flight lines spread across all latitudes. PROJ, tho.\n",
    "* [netCDF4](https://unidata.github.io/netcdf4-python/netCDF4/index.html) -- for writing a beautiful netCDF-4 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyproj import Transformer, crs\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the imports are from the Python 3 standard library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from io import TextIOWrapper\n",
    "from math import pi, sin, cos\n",
    "from datetime import datetime\n",
    "from os.path import basename, isfile\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inputs\n",
    "\n",
    "**Use the web map to select your desired flight line.** Copy the url to the tarball for L2 reflectance into the cell below. Or just use the url from my example.\n",
    "\n",
    "(Slightly more complicated logic for L1B data is not set up yet.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prm20160125t195943_refl.tar.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__url__ = \"ftp://prmuser:tHQxi3a5@avng.jpl.nasa.gov/y16_data/prm20160125t195943_refl.tar.gz\"\n",
    "__tar__ = basename(__url__)\n",
    "__tar__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 'prm20160125t195943_refl.tar.gz'. Skip download.\n"
     ]
    }
   ],
   "source": [
    "if isfile(__tar__):\n",
    "    print(f\"Found '{__tar__}'. Skip download.\")\n",
    "else:\n",
    "    print(f\"Downloading '{__url__}' ...\")\n",
    "    try:\n",
    "        urlretrieve(__url__, __tar__)\n",
    "    except Exception as e:\n",
    "        print(f\"Download fail!\")\n",
    "        raise e\n",
    "    else:\n",
    "        print(f\"Download success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tarfile's context manager to open the parent archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prm20160125t195943_corr_v1k',\n",
       " 'prm20160125t195943_corr_v1k/prm20160125t195943_corr_v1k_img',\n",
       " 'prm20160125t195943_corr_v1k/prm20160125t195943_corr_v1k_img.hdr',\n",
       " 'prm20160125t195943_corr_v1k/prm20160125t195943_README_v1k.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tarfile.open(__tar__, \"r\") as z:\n",
    "    tcontents = z.getnames()\n",
    "tcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll **call the path to the output netCDF `__out__`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prm20160125t195943_refl.nc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__out__ = __tar__.replace(\".tar.gz\", \".nc\")\n",
    "__out__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input (ENVI binary image)\n",
    "\n",
    "### header\n",
    "\n",
    "https://www.harrisgeospatial.com/docs/enviheaderfiles.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the header file from the `tar_contents` and read it as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _hpath = [t for t in tcontents if t.endswith(\"_img.hdr\")][0]\n",
    "except IndexError as e:\n",
    "    raise Exception(\"ERROR: No '.hdr' in source tarball. Exiting.\")\n",
    "else:\n",
    "    with tarfile.open(__tar__, \"r\") as z:\n",
    "        with z.extractfile(_hpath) as f:\n",
    "            hdr = TextIOWrapper(f, encoding=\"utf-8\", errors=\"ignore\").read()\n",
    "    print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a function that'll strip and describe the header text as a dictionary fairly reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split the header into a list of lines. Drop the first one.\n",
    "hlines = hdr.split(\"\\n\")[1:]\n",
    "\n",
    "# Parsing header in a few nested loops. First, split keys from values.\n",
    "hpairs = [l.split(\" = \") for l in hlines if \" = \" in l]\n",
    "\n",
    "# Then, format the resulting strings as a dictionary.\n",
    "hdict = {l[0]: l[1].strip() for l in hpairs}\n",
    "\n",
    "# Parse the 'map info' into a labeled array.\n",
    "hdict['map info'] = [k.strip() for k in hdict['map info'][1:-1].split(\" , \")]\n",
    "\n",
    "# Iterate over a few special header elements and parse further.\n",
    "for k in ['wavelength', 'fwhm', 'correction factors', 'smoothing factors']:\n",
    "    hdict[k] = [float(v.strip()) for v in hdict[k][1:-1].split(\",\")]\n",
    "\n",
    "hdr = hdict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the keys of the header dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['description',\n",
       " 'samples',\n",
       " 'lines',\n",
       " 'bands',\n",
       " 'header offset',\n",
       " 'file type',\n",
       " 'data type',\n",
       " 'interleave',\n",
       " 'byte order',\n",
       " 'map info',\n",
       " 'wavelength units',\n",
       " 'smoothing factors',\n",
       " 'data ignore value',\n",
       " 'wavelength',\n",
       " 'fwhm',\n",
       " 'correction factors']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hdr.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shape\n",
    "\n",
    "Check the shape of the 3-dimensional gridded dataset, and convert the sizes of the dimensions to integers while we're at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197, 246, 985)\n"
     ]
    }
   ],
   "source": [
    "samples = int(hdr['samples'])\n",
    "bands = int(hdr['bands'])\n",
    "lines = int(hdr['lines'])\n",
    "\n",
    "print((samples, bands, lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interleave\n",
    "\n",
    "Note the interleave types and the dimension orders. We need to reshape the giant 1-dimensional array that we read from the binary file in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197, 246, 985)\n"
     ]
    }
   ],
   "source": [
    "native_shape = {\n",
    "    'BSQ': (samples, lines, bands),  # Band Sequential\n",
    "    'BIP': (bands, samples, lines),  # Band Interleave by Pixel\n",
    "    'BIL': (samples, bands, lines),  # Band Interleave by Line\n",
    "}[hdr['interleave'].upper()]\n",
    "\n",
    "print(native_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BIL'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr['interleave'].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data type\n",
    "\n",
    "See the type map in the table on the ENVI header [documentation](https://www.harrisgeospatial.com/docs/enviheaderfiles.html). Also see the type codes given in the `numpy.dtypes` [documentation](https://numpy.org/doc/stable/reference/arrays.dtypes.html) (and [here](https://numpy.org/doc/stable/user/basics.types.html)).\n",
    "\n",
    "Determine the corresponding `numpy` data type of the binary array stored in the ENVI image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = {\n",
    "    '1': np.uint8,    # Byte: 8-bit unsigned integer\n",
    "    '2': np.int16,    # Integer: 16-bit signed integer\n",
    "    '3': np.int32,    # Long: 32-bit signed integer\n",
    "    '4': np.single,   # Floating-point: 32-bit single-precision\n",
    "    '5': np.double,   # Double-precision: 64-bit double-precision floating-point\n",
    "    '6': np.csingle,  # Complex: Real-imaginary pair of single-precision floating-point\n",
    "    '9': np.cdouble,  # Double-precision complex: Real-imaginary pair of double precision floating-point\n",
    "    '12': np.uint16,  # Unsigned integer: 16-bit\n",
    "    '13': np.uint32,  # Unsigned long integer: 32-bit\n",
    "    '14': np.int64,   # 64-bit long integer (signed)\n",
    "    '15': np.uint64,  # 64-bit unsigned long integer (unsigned)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look up the data type of the example PRISM L2 binary image file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr['data type'] = data_types[hdr['data type']]\n",
    "hdr['data type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### byte order\n",
    "\n",
    "The `byte order` field conveys the order of the bytes in integer, long integer, 64-bit integer, unsigned 64-bit integer, floating point, double precision, and complex data types. Use one of the following:\n",
    "\n",
    "* **0**: little endian; (Host (Intel) in the Header Info dialog) is least significant byte first (LSF) data (DEC and MS-DOS systems).\n",
    "* **1**: big endian; (Network (IEEE) in the Header Info dialog) is most significant byte first (MSF) data (all other platforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_orders = {\n",
    "    '0': \"<\",  # little-endian\n",
    "    '1': \">\",  # big-endian\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the byte order to the appropriate numpy encoding (prefixed to the data type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hbyteorder = byte_orders[hdr['byte order']]\n",
    "hbyteorder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map info\n",
    "\n",
    "Finally, let's label the `map info` data in the header data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_info_labels = {\n",
    "    0:  (\"Projection name\", str),\n",
    "    1:  (\"Reference (tie point) pixel x location (in file coordinates)\", int),\n",
    "    2:  (\"Reference (tie point) pixel y location (in file coordinates)\", int),\n",
    "    3:  (\"Pixel easting\", float),\n",
    "    4:  (\"Pixel northing\", float),\n",
    "    5:  (\"x pixel size\", float),\n",
    "    6:  (\"y pixel size\", float),\n",
    "    7:  (\"Projection zone (UTM only)\", int),\n",
    "    8:  (\"North or South (UTM only)\", str),\n",
    "    9:  (\"Datum\", str),\n",
    "    10: (\"Units\", str),\n",
    "    11: (\"Rotation\", lambda x: float(x.split(\"=\")[1])),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label the array of spatial characteristics and replace the `map info` key in the header data dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Projection name': 'UTM',\n",
       " 'Reference (tie point) pixel x location (in file coordinates)': 1,\n",
       " 'Reference (tie point) pixel y location (in file coordinates)': 1,\n",
       " 'Pixel easting': 478393.771278,\n",
       " 'Pixel northing': 2488191.16776,\n",
       " 'x pixel size': 10.3,\n",
       " 'y pixel size': 10.3,\n",
       " 'Projection zone (UTM only)': 19,\n",
       " 'North or South (UTM only)': 'South',\n",
       " 'Datum': 'WGS-84',\n",
       " 'Units': 'units=Meters',\n",
       " 'Rotation': -17.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr['map info'] = {v[0]: v[1](hdr['map info'][k]) for k, v in map_info_labels.items()}\n",
    "hdr['map info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ENVI header file encodes the grid structure using info shown above + the 3d array shape (x/sample, y/line, band).\n",
    "\n",
    "netCDF-4 files that use the CF-1.6+ grid mapping spec give the coverage for each pixel through the coordinate variables. We need to make *four* arrays of spatial coordinates to conform to grid mapping spec:\n",
    "\n",
    "* a 1d array of X coordinates in meters (UTM eastings),\n",
    "* a 1d array of Y coordinates in meters (UTM northings),\n",
    "* a 2d array of longitude coordinates in decimal degrees,\n",
    "* a 2d array of latitude coordinates in decimal degrees,\n",
    "\n",
    "Get the sizes of the X (sample), Y (line), and band dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197, 246, 985)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples, bands, lines = int(hdr['samples']), int(hdr['bands']), int(hdr['lines'])\n",
    "samples, bands, lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ENVI format provides for rotated grids. PRISM L1B and L2 should use one of these in common formats.\n",
    "\n",
    "**Important:** The header gives raster rotation in degrees, but we need it in radians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.29670597283903605"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation = (pi/180) * hdr['map info']['Rotation']\n",
    "rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the X,Y origin and resolution from the `map info` header field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478393.771278, 2488191.16776)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the X and Y position of the raster origin in meters.\n",
    "xorigin = hdr['map info']['Pixel easting']\n",
    "yorigin = hdr['map info']['Pixel northing']\n",
    "\n",
    "# Get the X and Y dimensions of the pixels in meters.\n",
    "xresolution = hdr['map info']['x pixel size']\n",
    "yresolution = hdr['map info']['y pixel size']\n",
    "\n",
    "(xorigin, yorigin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An affine transform describes the relationship between raster positions (sample, line) and georeferenced coordinates (x, y). \n",
    "\n",
    "GDAL's raster data model is how I make sense of this stuff [`*`](https://gdal.org/user/raster_data_model.html#affine-geotransform). It uses six coefficients to describe the transform (instead of a matrix) and isn't suitable for rotated grids by default. We need modified coefficients for items 1, 2, 4, and 5:\n",
    "\n",
    "```python\n",
    "0. x origin      # (The origin refers to top-left corner of top-left pixel, in this case.)\n",
    "1. x resolution \n",
    "2. x rotation\n",
    "3. y origin\n",
    "4. y rotation\n",
    "5. y resolution\n",
    "```\n",
    "\n",
    "Get the six coefficients like GDAL does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478398.921278,\n",
       " 9.849938986419266,\n",
       " 3.011428558644189,\n",
       " 2488186.01776,\n",
       " -3.011428558644189,\n",
       " 9.849938986419266)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = (\n",
    "    xorigin + xresolution/2, \n",
    "    cos(rotation)*xresolution,\n",
    "    -sin(rotation)*xresolution,\n",
    "    yorigin - yresolution/2, \n",
    "    sin(rotation)*yresolution,\n",
    "    cos(rotation)*yresolution,\n",
    ")\n",
    "\n",
    "gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the new coefficients match the ones returned by the [gdalinfo](https://gdal.org/programs/gdalinfo.html#gdalinfo) command line utility:\n",
    "\n",
    "```python\n",
    "$ gdalinfo prm20160125t195943_corr_v1k_img | grep -A 2 GeoTransform\n",
    "\n",
    "GeoTransform =\n",
    "  478393.771278, 9.849938986419266, -3.011428558644189\n",
    "  2488191.16776, -3.011428558644189, -9.849938986419266\n",
    "\n",
    "```\n",
    "\n",
    "Now the normal linear transform gives the UTM X,Y coordinates, as illustrated in this nice fig from the ENVI docs (source: [https://www.harrisgeospatial.com/docs/OverviewMapInformationInENVI.html#Standard](https://www.harrisgeospatial.com/docs/OverviewMapInformationInENVI.html#Standard)):\n",
    "\n",
    "<img src=https://www.harrisgeospatial.com/docs/html/images/GeorectifyImagery/map_transformation_standard.gif />\n",
    "\n",
    "Print the first/last items in each of the coordinate arrays + their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X(197):  478398.921 -  480329.509\n",
      "Y(985): 2488186.018 - 2478493.678\n"
     ]
    }
   ],
   "source": [
    "x = np.array([gt[0] + i*gt[1] for i in range(0, samples)])\n",
    "print(f\"X({len(x)}):  {round(x[0],3)} -  {round(x[-1],3)}\")\n",
    "\n",
    "y = np.array([gt[3] - i*gt[5] for i in range(0, lines)])\n",
    "print(f\"Y({len(y)}): {round(y[0],3)} - {round(y[-1],3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for the next step, get two 2-dimensional arrays of X and Y coordinates by expanding column- and row-wise to reference every pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 197)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2d, y2d = np.meshgrid(x, y)\n",
    "x2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### longitudes, latitudes\n",
    "\n",
    "Now we need to get two 2-dimensional arrays of longitudes and latitudes that coincide with the permuted X,Y positions. Here's the proj4 string that represents the appropriate UTM zone and north/south hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 'South', '+proj=utm +zone=19 +south +datum=WGS84 +units=m +no_defs']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone = hdr['map info']['Projection zone (UTM only)']\n",
    "hemi = hdr['map info']['North or South (UTM only)']\n",
    "\n",
    "# Format the proj4 string with the UTM zone and the ns hemisphere identifier.\n",
    "proj4 = f\"+proj=utm +zone={zone} +{hemi.lower()} +datum=WGS84 +units=m +no_defs\"\n",
    "\n",
    "[zone, hemi, proj4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create references to the native UTM coordinate system and to the geographic coordinate system based on WGS84 using `pyproj.Proj`. Apply the PROJ default transformations over the two new 2-d X and Y arrays to render the corresponding longitude and latitude arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985, 197)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init the transform based on source and target projections.\n",
    "xform = Transformer.from_crs(proj4, \"epsg:4326\")\n",
    "\n",
    "# Apply PROJ default transform utm >>> geo to all X,Y coordinates.\n",
    "lon, lat = xform.transform(x2d, y2d)\n",
    "\n",
    "lon.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the longitudes and latitudes reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-67.80776581256269, -69.51246610322195, -67.72069600218123, -69.4649394854862)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lon.min(), lat.min(), lon.max(), lat.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOOYAH! Now we can read the image data and write a netCDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read image\n",
    "\n",
    "Read the binary array from the file suffixed with `_img` (from inside the tarball). Make sure to pass the numpy data type as a keyword argument to `np.frombuffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9999., -9999., -9999., ..., -9999., -9999., -9999.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    _img = [t for t in tcontents if t.endswith(\"_img\")][0]\n",
    "except IndexError as e:\n",
    "    raise Exception(\"ERROR: No '.hdr' in source tarball. Exiting.\")\n",
    "else:\n",
    "    with tarfile.open(__tar__, \"r\") as z:\n",
    "         with z.extractfile(_img) as zimg:\n",
    "            arr = np.frombuffer(zimg.read(), dtype=hdr['data type'])\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the size of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47735070"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should equal the `samples * lines * bands` from the ENVI image header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples*bands*lines == arr.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the array to match dimensions ordered for BIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197, 246, 985)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = arr.reshape(native_shape)\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output (netCDF-4)\n",
    "\n",
    "Now we're ready to write all of that information to a new netCDF file. Here's a direct link to the `Dataset` init options: https://unidata.github.io/netcdf4-python/netCDF4/index.html#netCDF4.Dataset.__init__.\n",
    "\n",
    "Open the new dataset for writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): \n",
       "    variables(dimensions): \n",
       "    groups: "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset(\n",
    "    __out__, \n",
    "    mode=\"w\",           # Open in write mode.\n",
    "    clobber=True,       # Overwrite the existing file, if necessary.\n",
    "    format=\"NETCDF4\",   # Write the output file in netCDF-4 format.\n",
    "    parallel=False,     # Enable parallel read/write. (Must be built with MPI support.)\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global attributes\n",
    "\n",
    "This attribute configuration is just a first draft. They mostly follow the example [L1B and L2 datasets published by OBDAAC for CORAL](https://oceancolor.gsfc.nasa.gov/projects/prism-coral/).\n",
    "\n",
    "Make a dictionary to store the global attribute information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts = dict(\n",
    "    id                       = \"10.5067/PRISM/#\",\n",
    "    naming_authority         = \"gov.nasa.jpl.prism\",\n",
    "    license                  = \"https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/\",\n",
    "    project                  = \"NASA PRISM\",\n",
    "    project_url              = \"https://prism.jpl.nasa.gov/\",\n",
    "    institution              = \"NASA Jet Propulsion Laboratory\",\n",
    "    instrument               = \"PRISM (Portable Remote Imaging SpectroMeter)\",\n",
    "    platform                 = \"G-IV (Gulfstream-IV)\",\n",
    "    Conventions              = \"CF-1.7\",\n",
    "    keywords_vocabulary      = \"GCMD Science Keywords\",\n",
    "    standard_name_vocabulary = \"CF Standard Names v72\",\n",
    "    processing_version       = \"V1.0\",\n",
    "    product_version          = \"v1w2\",\n",
    "    product_name             = basename(__out__),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add creator metadata recommended by CF and ACDD Conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['creator_name']         = \"PRISM Science Team\"\n",
    "atts['creator_role']         = \"group\"\n",
    "atts['creator_url']          = \"https://prism.jpl.nasa.gov\"\n",
    "atts['creator_email']        = \"sarah.r.lundeen@jpl.nasa.gov\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add publisher metadata recommended by CF and ACDD Conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['publisher_name']       = \"PRISM Science Team\"\n",
    "atts['publisher_role']       = \"group\"\n",
    "atts['publisher_url']        = \"https://prism.jpl.nasa.gov\"\n",
    "atts['publisher_email']      = \"sarah.r.lundeen@jpl.nasa.gov\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a reference to the file write/update time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['date_created']         = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "atts['date_updated']         = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add any remaining file-dependent attributes that are recommended by CF and ACDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "atts['geospatial_lon_min']   = lon.min()\n",
    "atts['geospatial_lon_max']   = lon.max()\n",
    "atts['geospatial_lon_units'] = \"degrees_east\"\n",
    "atts['geospatial_lat_min']   = lat.min()\n",
    "atts['geospatial_lat_max']   = lat.max()\n",
    "atts['geospatial_lat_units'] = \"degrees_north\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Won't write any logic to determine a start/end range for now. Space/time search is pretty important for PRISM, as I'm sure any readers of this doc already know. So PODAAC would need to describe a representative observation period in those attributes to have complete metadata according to our requirements.\n",
    "\n",
    "Skip time attributes, then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atts['time_coverage_start'] = \"yyyy-mm-ddThh:mm:ssZ\"\n",
    "# atts['time_coverage_end']   = \"yyyy-mm-ddThh:mm:ssZ\"\n",
    "# atts['time_coverage_res']   = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimensions\n",
    "\n",
    "The `samples` and `lines` (both integers) are used to specify the size of the `x` and `y` dimensions in the output file, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): x(197), y(985), band(246)\n",
       "    variables(dimensions): \n",
       "    groups: "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdim = ds.createDimension('x', size=samples)\n",
    "ydim = ds.createDimension('y', size=lines)\n",
    "bdim = ds.createDimension('band', size=bands)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll add another dimension `nv` to accomodate a paired axis (for bounding the coordinates axes that label a non-standard dimension, if that makes sense...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nvdim = ds.createDimension('nv', size=2)\n",
    "#nvdim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coordinate variables\n",
    "\n",
    "I typically write the coordinate variables next, right after the dimensions are defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spatial\n",
    "\n",
    "We'll store spatial coordinates at the top of the file, in the dataset root group. CF isn't clear about whether or not this is a requirement. So we play it safe.\n",
    "\n",
    "Add the `x` and `y` coordinates and attributes as new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): x(197), y(985), band(246)\n",
       "    variables(dimensions): float64 x(x), float64 y(y)\n",
       "    groups: "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_var = ds.createVariable('x', 'f8', ('x'), fill_value=None)\n",
    "x_var.units = \"m\"\n",
    "x_var.axis = \"X\"\n",
    "x_var.standard_name = \"projection_x_coordinate\"\n",
    "x_var.long_name = \"x coordinate of projection\"\n",
    "x_var[:] = x\n",
    "\n",
    "y_var = ds.createVariable('y', 'f8', ('y'), fill_value=None)\n",
    "y_var.units = \"m\"\n",
    "y_var.axis = \"Y\"\n",
    "y_var.standard_name = \"projection_y_coordinate\"\n",
    "y_var.long_name = \"y coordinate of projection\"\n",
    "y_var[:] = y\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longitude and latitude arrays are 2-dimensional. Make sure to order `y,x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): x(197), y(985), band(246)\n",
       "    variables(dimensions): float64 x(x), float64 y(y), float32 lat(y, x), float32 lon(y, x)\n",
       "    groups: "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_var = ds.createVariable('lat', 'f4', ('y', 'x'), fill_value=None)\n",
    "lat_var.units = \"degrees_north\"\n",
    "lat_var.standard_name = \"latitude\"\n",
    "lat_var.long_name = \"latitude coordinate\"\n",
    "lat_var[:,:] = lat\n",
    "\n",
    "lon_var = ds.createVariable('lon', 'f4', ('y', 'x'), fill_value=None)\n",
    "lon_var.units = \"degrees_east\"\n",
    "lon_var.standard_name = \"longitude\"\n",
    "lon_var.long_name = \"longitude coordinate\"\n",
    "lon_var[:,:] = lon\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now the sometimes-tricky CF grid mapping requirements...***\n",
    "\n",
    "Let's do the CF grid mapping variable now. It's be assigned attributes based on the spec for the `transverse_mercator` standard grid mapping (CF-1.6+). And [pyproj](#) is so legit for this. It can dump the CF standard attributes as a Python dictionary.\n",
    "\n",
    "You can find the grid mapping spec in clean format here: https://cfconventions.org/Data/cf-conventions/cf-conventions-1.7/build/apf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'crs_wkt': 'PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"World Geodetic System 1984\",ELLIPSOID[\"WGS 84\",6378137,298.257223563,LENGTHUNIT[\"metre\",1]],ID[\"EPSG\",6326]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8901]]],CONVERSION[\"UTM zone 19S\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-69,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",500000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",10000000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]],ID[\"EPSG\",17019]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]]]',\n",
       " 'semi_major_axis': 6378137.0,\n",
       " 'semi_minor_axis': 6356752.314245179,\n",
       " 'inverse_flattening': 298.257223563,\n",
       " 'reference_ellipsoid_name': 'WGS 84',\n",
       " 'longitude_of_prime_meridian': 0.0,\n",
       " 'prime_meridian_name': 'Greenwich',\n",
       " 'geographic_crs_name': 'unknown',\n",
       " 'horizontal_datum_name': 'World Geodetic System 1984',\n",
       " 'projected_crs_name': 'unknown',\n",
       " 'grid_mapping_name': 'transverse_mercator',\n",
       " 'latitude_of_projection_origin': 0.0,\n",
       " 'longitude_of_central_meridian': -69.0,\n",
       " 'false_easting': 500000.0,\n",
       " 'false_northing': 10000000.0,\n",
       " 'scale_factor_at_central_meridian': 0.9996}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_mapping_atts = crs.CRS(proj4).to_cf()\n",
    "grid_mapping_atts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the CF-required grid_mapping variable and set its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "|S1 UTM_Projection()\n",
       "    crs_wkt: PROJCRS[\"unknown\",BASEGEOGCRS[\"unknown\",DATUM[\"World Geodetic System 1984\",ELLIPSOID[\"WGS 84\",6378137,298.257223563,LENGTHUNIT[\"metre\",1]],ID[\"EPSG\",6326]],PRIMEM[\"Greenwich\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8901]]],CONVERSION[\"UTM zone 19S\",METHOD[\"Transverse Mercator\",ID[\"EPSG\",9807]],PARAMETER[\"Latitude of natural origin\",0,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8801]],PARAMETER[\"Longitude of natural origin\",-69,ANGLEUNIT[\"degree\",0.0174532925199433],ID[\"EPSG\",8802]],PARAMETER[\"Scale factor at natural origin\",0.9996,SCALEUNIT[\"unity\",1],ID[\"EPSG\",8805]],PARAMETER[\"False easting\",500000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8806]],PARAMETER[\"False northing\",10000000,LENGTHUNIT[\"metre\",1],ID[\"EPSG\",8807]],ID[\"EPSG\",17019]],CS[Cartesian,2],AXIS[\"(E)\",east,ORDER[1],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]],AXIS[\"(N)\",north,ORDER[2],LENGTHUNIT[\"metre\",1,ID[\"EPSG\",9001]]]]\n",
       "    semi_major_axis: 6378137.0\n",
       "    semi_minor_axis: 6356752.314245179\n",
       "    inverse_flattening: 298.257223563\n",
       "    reference_ellipsoid_name: WGS 84\n",
       "    longitude_of_prime_meridian: 0.0\n",
       "    prime_meridian_name: Greenwich\n",
       "    geographic_crs_name: unknown\n",
       "    horizontal_datum_name: World Geodetic System 1984\n",
       "    projected_crs_name: unknown\n",
       "    grid_mapping_name: transverse_mercator\n",
       "    latitude_of_projection_origin: 0.0\n",
       "    longitude_of_central_meridian: -69.0\n",
       "    false_easting: 500000.0\n",
       "    false_northing: 10000000.0\n",
       "    scale_factor_at_central_meridian: 0.9996\n",
       "    _CoordinateTransformType: Projection\n",
       "    _CoordinateAxisTypes: GeoY GeoX\n",
       "unlimited dimensions: \n",
       "current shape = ()\n",
       "filling on, default _FillValue of \u0000 used"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crs_var = ds.createVariable(\"UTM_Projection\", \"|S1\")\n",
    "crs_var.setncatts(grid_mapping_atts)\n",
    "crs_var._CoordinateTransformType = \"Projection\";\n",
    "crs_var._CoordinateAxisTypes = \"GeoY GeoX\";\n",
    "\n",
    "crs_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bands\n",
    "\n",
    "The band dimension `band` coordinates are coming from the ENVI header's `wavelength` field. There are a few other sensor-related variables, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "float32 wavelength(band)\n",
       "    long_name: wavelengths of band centers\n",
       "    units: nm\n",
       "    valid_min: 350.0\n",
       "    valid_max: 1050.0\n",
       "unlimited dimensions: \n",
       "current shape = (246,)\n",
       "filling on, default _FillValue of 9.969209968386869e+36 used"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the array of wavelengths taken from the header.\n",
    "wave = np.array(hdr['wavelength'])\n",
    "\n",
    "# Create a wavelength variable; add the attributes and data.\n",
    "wave_var = ds.createVariable('wavelength', 'f4', ('band'), fill_value=None)\n",
    "wave_var.long_name = \"wavelengths of band centers\"\n",
    "wave_var.units = \"nm\"\n",
    "wave_var.valid_min = 350.0\n",
    "wave_var.valid_max = 1050.0\n",
    "wave_var[:] = wave\n",
    "\n",
    "wave_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good habit to bound the coordinates that describe dimensions without standard encodings. I don't know how to derive the precise band coverages, so here I'll just do something super dumb and +- 1 from each band center to create a placeholder 2d array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make a 2d array of wavelength_bnds coordinates.\n",
    "# wavebnds = np.array([[w-1., w+1.] for w in wave])\n",
    "\n",
    "# # Create a 'wavelength_bnds' variable to bound the sensor bands.\n",
    "# wavebnds_var = ds.createVariable(\"wavelength_bnds\", \"f4\", (\"band\", \"nv\"), fill_value=None)\n",
    "# wavebnds_var.units = \"nm\"\n",
    "# wavebnds_var[:,:] = wavebnds\n",
    "\n",
    "# wavebnds_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Re-visit the `fwhm` data (band full-width half-maximums). Array size doesn't match the other two (*wavelength*, *correction factors*). Ask M. Gierach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fwhm_var = ds.createVariable('fwhm', 'float32', ('band'))\n",
    "# fwhm_var.long_name = \"band full-width half-maximums\"\n",
    "# fwhm_var.units = \"nm\"\n",
    "# fwhm_var.valid_min = 349.9\n",
    "# fwhm_var.valid_max = 1053.5\n",
    "# fwhm_var[:] = np.array(hdr['fwhm'])\n",
    "\n",
    "# fwhm_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know what these *correction factors* describe. But they fit the band dimension *band*, so we add them as a sensor variable as assume they're unitless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the array so we can get real min, max to bound this variable.\n",
    "corr = np.array(hdr['correction factors'])\n",
    "\n",
    "# Make the 'correction factors' variable.\n",
    "corr_var = ds.createVariable('correction_factors', 'float32', ('band'))\n",
    "corr_var.long_name = \"correction factors\"\n",
    "corr_var.units = \"unitless\"\n",
    "corr_var.valid_min = corr.min()\n",
    "corr_var.valid_max = corr.max()\n",
    "corr_var[:] = corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data\n",
    "\n",
    "This next cell simply selects the attributes that are dependent on processing levels and updates the global attributes as needed. Hang on to the data variable attributes `data_atts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today we're translating 'reflectance' data from ENVI image format to netCDF-4.\n"
     ]
    }
   ],
   "source": [
    "if \"_refl\" in __tar__:\n",
    "    atts.update({'title': \"PRISM Level-2 Reflectance\", 'processing_level': \"L2\"})\n",
    "    data_name = \"reflectance\"\n",
    "    data_atts = {\n",
    "        'long_name': \"reflectance\",\n",
    "        'units': \"unitless\",\n",
    "        'valid_min': 0.0,\n",
    "        'valid_max': 1000.0,\n",
    "        'grid_mapping': 'UTM_Projection',\n",
    "        'coordinates': \"lat lon\",\n",
    "    }\n",
    "else:\n",
    "    atts.update({'title': \"PRISM Level-1B Orthocorrected Radiance\", 'processing_level': \"L1B\"})\n",
    "    data_name = \"radiance\"\n",
    "    data_atts = {\n",
    "        'long_name': \"at-sensor radiance\",\n",
    "        'units': \"uW cm^-2 nm^-1 sr^-1\",\n",
    "        'valid_min': 0.0,\n",
    "        'valid_max': 800.0,\n",
    "        'grid_mapping': 'UTM_Projection',\n",
    "        'coordinates': \"lat lon\",\n",
    "    }\n",
    "\n",
    "print(f\"Today we're translating '{data_name}' data from ENVI image format to netCDF-4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data variable can often be compressed for some space savings, so the setup is a bit different this time. After the variable name, set\n",
    "\n",
    "1. Data Type will be `f4` for most data variables in the PRISM outputs, I think.\n",
    "2. Dimensions should be `y`, `x`, `band`, in that order.\n",
    "3. ZLIB will attempt to apply compression.\n",
    "4. The default compression level `complevel` is four, on a scale from 0-9.\n",
    "5. The `_FillValue` attribute MUST be set when you make the variable. It cannot be set like other attributes. I can explain the details for any interested readers outside of this notebook.\n",
    "\n",
    "Make the new variable for the observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "float32 reflectance(band, y, x)\n",
       "    _FillValue: -9999.0\n",
       "    long_name: reflectance\n",
       "    units: unitless\n",
       "    valid_min: 0.0\n",
       "    valid_max: 1000.0\n",
       "    grid_mapping: UTM_Projection\n",
       "    coordinates: lat lon\n",
       "unlimited dimensions: \n",
       "current shape = (246, 985, 197)\n",
       "filling on"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure fill value parsed from ENVI header is same dtype as the data array.\n",
    "data_type = hdr['data type']\n",
    "fill_value = data_type( hdr['data ignore value'] )\n",
    "\n",
    "# Create the new variable in the 'observation_data' group.\n",
    "data = ds.createVariable(\n",
    "    data_name,\n",
    "    data_type,\n",
    "    dimensions=('band', 'y', 'x'),\n",
    "    zlib=True,\n",
    "    complevel=4,\n",
    "    fill_value=fill_value,\n",
    ")\n",
    "\n",
    "# Set the attributes for the variable.\n",
    "data.setncatts(data_atts)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now insert the array into the variable, expanding to all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "float32 reflectance(band, y, x)\n",
       "    _FillValue: -9999.0\n",
       "    long_name: reflectance\n",
       "    units: unitless\n",
       "    valid_min: 0.0\n",
       "    valid_max: 1000.0\n",
       "    grid_mapping: UTM_Projection\n",
       "    coordinates: lat lon\n",
       "unlimited dimensions: \n",
       "current shape = (246, 985, 197)\n",
       "filling on"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,:,:] = arr.reshape((bands, lines, samples))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, set global attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.setncatts(atts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the dataset and you're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have the netCDF-4 C library utilities installed, you can call ncdump to eyeball the formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netcdf prm20160125t195943_refl {\r\n",
      "dimensions:\r\n",
      "\tx = 197 ;\r\n",
      "\ty = 985 ;\r\n",
      "\tband = 246 ;\r\n",
      "variables:\r\n",
      "\tdouble x(x) ;\r\n",
      "\t\tx:units = \"m\" ;\r\n",
      "\t\tx:axis = \"X\" ;\r\n",
      "\t\tx:standard_name = \"projection_x_coordinate\" ;\r\n",
      "\t\tx:long_name = \"x coordinate of projection\" ;\r\n",
      "\tdouble y(y) ;\r\n",
      "\t\ty:units = \"m\" ;\r\n",
      "\t\ty:axis = \"Y\" ;\r\n",
      "\t\ty:standard_name = \"projection_y_coordinate\" ;\r\n",
      "\t\ty:long_name = \"y coordinate of projection\" ;\r\n",
      "\tfloat lat(y, x) ;\r\n",
      "\t\tlat:units = \"degrees_north\" ;\r\n",
      "\t\tlat:standard_name = \"latitude\" ;\r\n",
      "\t\tlat:long_name = \"latitude coordinate\" ;\r\n",
      "\tfloat lon(y, x) ;\r\n",
      "\t\tlon:units = \"degrees_east\" ;\r\n",
      "\t\tlon:standard_name = \"longitude\" ;\r\n",
      "\t\tlon:long_name = \"longitude coordinate\" ;\r\n",
      "\tchar UTM_Projection ;\r\n",
      "\t\tUTM_Projection:crs_wkt = \"PROJCRS[\\\"unknown\\\",BASEGEOGCRS[\\\"unknown\\\",DATUM[\\\"World Geodetic System 1984\\\",ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,LENGTHUNIT[\\\"metre\\\",1]],ID[\\\"EPSG\\\",6326]],PRIMEM[\\\"Greenwich\\\",0,ANGLEUNIT[\\\"degree\\\",0.0174532925199433],ID[\\\"EPSG\\\",8901]]],CONVERSION[\\\"UTM zone 19S\\\",METHOD[\\\"Transverse Mercator\\\",ID[\\\"EPSG\\\",9807]],PARAMETER[\\\"Latitude of natural origin\\\",0,ANGLEUNIT[\\\"degree\\\",0.0174532925199433],ID[\\\"EPSG\\\",8801]],PARAMETER[\\\"Longitude of natural origin\\\",-69,ANGLEUNIT[\\\"degree\\\",0.0174532925199433],ID[\\\"EPSG\\\",8802]],PARAMETER[\\\"Scale factor at natural origin\\\",0.9996,SCALEUNIT[\\\"unity\\\",1],ID[\\\"EPSG\\\",8805]],PARAMETER[\\\"False easting\\\",500000,LENGTHUNIT[\\\"metre\\\",1],ID[\\\"EPSG\\\",8806]],PARAMETER[\\\"False northing\\\",10000000,LENGTHUNIT[\\\"metre\\\",1],ID[\\\"EPSG\\\",8807]],ID[\\\"EPSG\\\",17019]],CS[Cartesian,2],AXIS[\\\"(E)\\\",east,ORDER[1],LENGTHUNIT[\\\"metre\\\",1,ID[\\\"EPSG\\\",9001]]],AXIS[\\\"(N)\\\",north,ORDER[2],LENGTHUNIT[\\\"metre\\\",1,ID[\\\"EPSG\\\",9001]]]]\" ;\r\n",
      "\t\tUTM_Projection:semi_major_axis = 6378137. ;\r\n",
      "\t\tUTM_Projection:semi_minor_axis = 6356752.31424518 ;\r\n",
      "\t\tUTM_Projection:inverse_flattening = 298.257223563 ;\r\n",
      "\t\tUTM_Projection:reference_ellipsoid_name = \"WGS 84\" ;\r\n",
      "\t\tUTM_Projection:longitude_of_prime_meridian = 0. ;\r\n",
      "\t\tUTM_Projection:prime_meridian_name = \"Greenwich\" ;\r\n",
      "\t\tUTM_Projection:geographic_crs_name = \"unknown\" ;\r\n",
      "\t\tUTM_Projection:horizontal_datum_name = \"World Geodetic System 1984\" ;\r\n",
      "\t\tUTM_Projection:projected_crs_name = \"unknown\" ;\r\n",
      "\t\tUTM_Projection:grid_mapping_name = \"transverse_mercator\" ;\r\n",
      "\t\tUTM_Projection:latitude_of_projection_origin = 0. ;\r\n",
      "\t\tUTM_Projection:longitude_of_central_meridian = -69. ;\r\n",
      "\t\tUTM_Projection:false_easting = 500000. ;\r\n",
      "\t\tUTM_Projection:false_northing = 10000000. ;\r\n",
      "\t\tUTM_Projection:scale_factor_at_central_meridian = 0.9996 ;\r\n",
      "\t\tUTM_Projection:_CoordinateTransformType = \"Projection\" ;\r\n",
      "\t\tUTM_Projection:_CoordinateAxisTypes = \"GeoY GeoX\" ;\r\n",
      "\tfloat wavelength(band) ;\r\n",
      "\t\twavelength:long_name = \"wavelengths of band centers\" ;\r\n",
      "\t\twavelength:units = \"nm\" ;\r\n",
      "\t\twavelength:valid_min = 350.f ;\r\n",
      "\t\twavelength:valid_max = 1050.f ;\r\n",
      "\tfloat correction_factors(band) ;\r\n",
      "\t\tcorrection_factors:long_name = \"correction factors\" ;\r\n",
      "\t\tcorrection_factors:units = \"unitless\" ;\r\n",
      "\t\tcorrection_factors:valid_min = 1.f ;\r\n",
      "\t\tcorrection_factors:valid_max = 1.f ;\r\n",
      "\tfloat reflectance(band, y, x) ;\r\n",
      "\t\treflectance:_FillValue = -9999.f ;\r\n",
      "\t\treflectance:long_name = \"reflectance\" ;\r\n",
      "\t\treflectance:units = \"unitless\" ;\r\n",
      "\t\treflectance:valid_min = 0. ;\r\n",
      "\t\treflectance:valid_max = 1000. ;\r\n",
      "\t\treflectance:grid_mapping = \"UTM_Projection\" ;\r\n",
      "\t\treflectance:coordinates = \"lat lon\" ;\r\n",
      "\r\n",
      "// global attributes:\r\n",
      "\t\t:id = \"10.5067/PRISM/#\" ;\r\n",
      "\t\t:naming_authority = \"gov.nasa.jpl.prism\" ;\r\n",
      "\t\t:license = \"https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/\" ;\r\n",
      "\t\t:project = \"NASA PRISM\" ;\r\n",
      "\t\t:project_url = \"https://prism.jpl.nasa.gov/\" ;\r\n",
      "\t\t:institution = \"NASA Jet Propulsion Laboratory\" ;\r\n",
      "\t\t:instrument = \"PRISM (Portable Remote Imaging SpectroMeter)\" ;\r\n",
      "\t\t:platform = \"G-IV (Gulfstream-IV)\" ;\r\n",
      "\t\t:Conventions = \"CF-1.7\" ;\r\n",
      "\t\t:keywords_vocabulary = \"GCMD Science Keywords\" ;\r\n",
      "\t\t:standard_name_vocabulary = \"CF Standard Names v72\" ;\r\n",
      "\t\t:processing_version = \"V1.0\" ;\r\n",
      "\t\t:product_version = \"v1w2\" ;\r\n",
      "\t\t:product_name = \"prm20160125t195943_refl.nc\" ;\r\n",
      "\t\t:creator_name = \"PRISM Science Team\" ;\r\n",
      "\t\t:creator_role = \"group\" ;\r\n",
      "\t\t:creator_url = \"https://prism.jpl.nasa.gov\" ;\r\n",
      "\t\t:creator_email = \"sarah.r.lundeen@jpl.nasa.gov\" ;\r\n",
      "\t\t:publisher_name = \"PRISM Science Team\" ;\r\n",
      "\t\t:publisher_role = \"group\" ;\r\n",
      "\t\t:publisher_url = \"https://prism.jpl.nasa.gov\" ;\r\n",
      "\t\t:publisher_email = \"sarah.r.lundeen@jpl.nasa.gov\" ;\r\n",
      "\t\t:date_created = \"2020-08-21T17:34:33Z\" ;\r\n",
      "\t\t:date_updated = \"2020-08-21T17:34:33Z\" ;\r\n",
      "\t\t:geospatial_lon_min = -67.8077658125627 ;\r\n",
      "\t\t:geospatial_lon_max = -67.7206960021812 ;\r\n",
      "\t\t:geospatial_lon_units = \"degrees_east\" ;\r\n",
      "\t\t:geospatial_lat_min = -69.512466103222 ;\r\n",
      "\t\t:geospatial_lat_max = -69.4649394854862 ;\r\n",
      "\t\t:geospatial_lat_units = \"degrees_north\" ;\r\n",
      "\t\t:title = \"PRISM Level-2 Reflectance\" ;\r\n",
      "\t\t:processing_level = \"L2\" ;\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!ncdump -h $__out__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## appendix\n",
    "\n",
    "### links\n",
    "\n",
    "See the links below for documentation about the relevant metadata specifications:\n",
    "\n",
    "* [Climate and Forecast Conventions version 1.x](#)\n",
    "* [Attribute Conventions for Data Discovery version 1.x](#)\n",
    "* [NetCDF Attribute Convention for Dataset Discovery](https://www.unidata.ucar.edu/software/netcdf-java/v4.6/metadata/DataDiscoveryAttConvention.html)\n",
    "* [standard_name_vocabulary](https://www.unidata.ucar.edu/software/netcdf-java/v4.6/metadata/DataDiscoveryAttConvention.html#standard_name_vocabulary_Attribute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests\n",
    "\n",
    "**Tests aren't implemented yet.** In fact, this workflow was implemented from memory without any end-to-end testing, so there are almost certainly bugs. Please let me validate before using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!ncdump -h \"docs/prm20160125t195943_corr_v1k_img.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xarray as xr\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1 = xr.open_dataset(\"docs/prm20160125t195943_corr_v1k_img.nc\")\n",
    "#ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.Band200.plot(figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2 = xr.open_dataset(\"prm20160125t195943_refl.nc\")\n",
    "#ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2.lat.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1 = ds1.x.data\n",
    "#x1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x2 = ds2.x.data\n",
    "#x2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1[0], x2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds2['reflectance'].isel(band=200).plot(figsize=(14,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds1.close(), ds2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.48px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
